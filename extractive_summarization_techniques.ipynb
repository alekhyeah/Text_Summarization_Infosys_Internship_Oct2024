{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkDdIhNIlxSLn1W35boUBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor0327/Text_Summarization_Infosys_Internship_Oct2024/blob/BandariRohith/extractive_summarization_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import textwrap\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from nltk.cluster.util import cosine_distance\n",
        "!pip install evaluate\n",
        "!pip install rouge_score  # Install the rouge_score package\n",
        "import evaluate  # Importing the evaluate library for ROUGE score\n",
        "\n",
        "# Step 2: Define the summarization function\n",
        "def summarize_text(text, algorithm=\"frequency\", num_sentences=3, additional_stopwords=None):\n",
        "    # Load the stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Add any additional stopwords provided by the user\n",
        "    if additional_stopwords:\n",
        "        stop_words.update(additional_stopwords)\n",
        "\n",
        "    # Frequency-based summarization\n",
        "    if algorithm == \"frequency\":\n",
        "        # Tokenize words and remove stopwords\n",
        "        words = word_tokenize(text.lower())\n",
        "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "        # Calculate word frequencies\n",
        "        freq_dist = FreqDist(filtered_words)\n",
        "\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Assign scores to sentences\n",
        "        sentence_scores = {}\n",
        "        for sentence in sentences:\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in freq_dist:\n",
        "                    if sentence not in sentence_scores:\n",
        "                        sentence_scores[sentence] = freq_dist[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += freq_dist[word]\n",
        "\n",
        "        # Sort and select the top sentences\n",
        "        summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "        summary = ' '.join(summary_sentences)\n",
        "\n",
        "    # TextRank-based summarization\n",
        "    elif algorithm == \"textrank\":\n",
        "        # Preprocess the sentences\n",
        "        def preprocess_sentences(sentences):\n",
        "            processed_sentences = []\n",
        "            for sentence in sentences:\n",
        "                words = word_tokenize(sentence.lower())\n",
        "                words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "                processed_sentences.append(words)\n",
        "            return processed_sentences\n",
        "\n",
        "        # Function to calculate cosine similarity between two sentences\n",
        "        def sentence_similarity(sent1, sent2):\n",
        "            all_words = list(set(sent1 + sent2))\n",
        "            vector1 = [0] * len(all_words)\n",
        "            vector2 = [0] * len(all_words)\n",
        "            for w in sent1:\n",
        "                vector1[all_words.index(w)] += 1\n",
        "            for w in sent2:\n",
        "                vector2[all_words.index(w)] += 1\n",
        "            return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "        # Function to build similarity matrix\n",
        "        def build_similarity_matrix(sentences):\n",
        "            similarity_matrix = [[0 for _ in range(len(sentences))] for _ in range(len(sentences))]\n",
        "            for i in range(len(sentences)):\n",
        "                for j in range(len(sentences)):\n",
        "                    if i != j:\n",
        "                        similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
        "            return similarity_matrix\n",
        "\n",
        "        # Tokenize and preprocess sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "        processed_sentences = preprocess_sentences(sentences)\n",
        "\n",
        "        # Build the similarity matrix and apply TextRank\n",
        "        similarity_matrix = build_similarity_matrix(processed_sentences)\n",
        "        nx_graph = nx.from_numpy_array(np.array(similarity_matrix))\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "\n",
        "        # Rank and extract the top sentences\n",
        "        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "        summary = ' '.join([ranked_sentences[i][1] for i in range(num_sentences)])\n",
        "\n",
        "    # Wrap the summary for readability\n",
        "    wrapped_summary = textwrap.fill(summary, width=80)\n",
        "    return wrapped_summary\n",
        "\n",
        "# Step 3: Define the function to calculate ROUGE score\n",
        "def calculate_rouge_score(reference, summary):\n",
        "    rouge = evaluate.load('rouge')\n",
        "    scores = rouge.compute(predictions=[summary], references=[reference])\n",
        "    return scores\n",
        "\n",
        "# Step 4: Get input from the user\n",
        "text = input(\"Enter the text to summarize: \")\n",
        "algorithm_choice = input(\"Enter summarization algorithm (frequency or textrank): \").strip().lower()\n",
        "num_sentences = int(input(\"Enter the number of sentences for the summary: \"))\n",
        "additional_stopwords = input(\"Enter any additional stopwords separated by commas (or press Enter to skip): \")\n",
        "additional_stopwords = additional_stopwords.split(',') if additional_stopwords else None\n",
        "\n",
        "# Step 5: Generate and print the summary\n",
        "summary = summarize_text(text, algorithm=algorithm_choice, num_sentences=num_sentences, additional_stopwords=additional_stopwords)\n",
        "print(\"\\nSummary:\\n\", summary)\n",
        "\n",
        "# Step 6: Calculate and print the ROUGE score\n",
        "rouge_scores = calculate_rouge_score(text, summary)\n",
        "print(\"\\nROUGE Scores:\\n\", rouge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzB4nFFK4Oy_",
        "outputId": "87761d58-0038-4253-a2f1-b90e07b302fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
            "Enter the text to summarize: Text summarization is an NLP process that focuses on reducing the amount of text from a given input while at the same time preserving key information and contextual meaning. With the amount of time and resources required for manual summarization, it's no surprise that automatic summarization with NLP has grown across a number of different use cases for many different document lengths. The summarization space has grown rapidly with a new focus on handling super large text inputs to summarize down into a few lines. The increased demand for the summarization of longer documents such as news articles and research papers has driven the growth in the space.  ‍  The key changes that have led to the new push in long text summarization are the introduction of transformer models such as BERT and GPT-3 that can handle much longer input sequences of text in a single run and a new understanding of chunking algorithms. Past architectures such as LSTMs or RNNs were not as efficient nor as accurate as these transformer based models, which made long document summarization much harder. The growth in understanding of how to build and use chunking algorithms that keep the structure of contextual information and reduce data variance at runtime has been key as well.  ‍  Difference Between Large & Small Text Summarization Packing all the contextual information from a document into a short summary is much harder with long text. If our summary has to be say 5 sentences max it is much harder to decide what information is valuable enough to be added with 500 words vs 50,000 words.  ‍  Chunking algorithms are often required, but they do grow the data variance coverage a model must have to be accurate. Chunking algorithms control how much of the larger document we pass into a summarizer based on the max tokens the model allows and parameters we’ve set. The new dynamic nature of the input data means our data variance is much larger than what is seen with smaller text.  ‍  Longer documents often have much more internal data variance and swings in the information. Use causes such as blog posts, interviews, transcripts and more have multiple swings in the dialog that make it harder to understand what contextual information is valuable for the summary. Models have to learn a much deeper relationship between specific keywords, topics, and phrases as the text grows.  ‍  There are two main types of summarization that are used as the baseline for any enhanced versions of summarization - Extractive and abstractive. They focus on how the key information found in the input text is reconstructed in the generated summary in their own ways. Both of these methods have their own unique challenges that pop up when looking at using longer text.    ‍  4 Key Methods of Long Text Summarization There are a number of different methods of summarizing long document text using various architectures and frameworks. We’ll look at some of the most popular ones used today and the various use cases that we have seen them perform exceptionally well for.  ‍  Long Text Summarization with GPT-4 The release of GPT-4 has changed two key aspects of long text summarization and has various pros and cons. Most customers immediately think that GPT-4 is the route to go right now no matter the use case or documents they want to process, but there are a number of things you want to consider before making this switch. Let’s talk about the key aspects GPT-4 has changed:  ‍  Chunking Its no secret that GPT-4 has a larger prompt size then GPT-3 and many other models. This in theory means that we don’t need to chunk the document into as many text blocks as previously needed. While this is true, there are a number of different things to consider using this extra space for outside of just having less chunks, as in my experience less chunks does not affect summarization as much as customers believe it will, and these additional tokens can be used for other more valuable prompt operations.  ‍  Further understanding of instructions GPT-4 has improved quite a bit at following more complex instructions or multi-step instructions. Instructions no longer have to be provided in a simpler structure and things like Chain of Thought and Tree of Thoughts have expanded this even further. We now use much more complex instructions even before getting to prompt examples.\n",
            "Enter summarization algorithm (frequency or textrank): textrank\n",
            "Enter the number of sentences for the summary: 5\n",
            "Enter any additional stopwords separated by commas (or press Enter to skip): \n",
            "\n",
            "Summary:\n",
            " ‍  Difference Between Large & Small Text Summarization Packing all the\n",
            "contextual information from a document into a short summary is much harder with\n",
            "long text. ‍  The key changes that have led to the new push in long text\n",
            "summarization are the introduction of transformer models such as BERT and GPT-3\n",
            "that can handle much longer input sequences of text in a single run and a new\n",
            "understanding of chunking algorithms. ‍  Long Text Summarization with GPT-4 The\n",
            "release of GPT-4 has changed two key aspects of long text summarization and has\n",
            "various pros and cons. ‍  4 Key Methods of Long Text Summarization There are a\n",
            "number of different methods of summarizing long document text using various\n",
            "architectures and frameworks. Text summarization is an NLP process that focuses\n",
            "on reducing the amount of text from a given input while at the same time\n",
            "preserving key information and contextual meaning.\n",
            "\n",
            "ROUGE Scores:\n",
            " {'rouge1': 0.3355855855855856, 'rouge2': 0.3250564334085779, 'rougeL': 0.22072072072072074, 'rougeLsum': 0.30180180180180183}\n"
          ]
        }
      ]
    }
  ]
}