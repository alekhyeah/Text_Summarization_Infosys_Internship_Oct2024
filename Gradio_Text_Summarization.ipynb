{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEnvT0coH3UOpnwujoZecZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gradio --quiet\n","!pip install sumy --quiet\n","\n","from google.colab import userdata\n","import gradio as gr\n","\n","# Define your summarization functions\n","def extractive_summarization_frequency(txt, target_word_count, min_sentence_word_count=10):\n","    import nltk\n","    import heapq\n","    from nltk.corpus import stopwords\n","    from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","    nltk.download('punkt')\n","    nltk.download('stopwords')\n","\n","    def summarize_text(text, target_word_count, min_sentence_word_count):\n","        # Tokenize sentences\n","        sentences = sent_tokenize(text)\n","\n","        # Preprocess text to filter out non-alphabetic words and stopwords\n","        def preprocess_text(text):\n","            processed_words = []\n","            for word in word_tokenize(text):\n","                if word.isalpha():\n","                    processed_words.append(word.lower())\n","            return processed_words\n","\n","        words = preprocess_text(text)\n","\n","        stop_words = set(stopwords.words('english'))\n","        filtered_words = [word for word in words if word not in stop_words]\n","\n","        # Calculate word frequencies\n","        word_frequencies = {}\n","        for word in filtered_words:\n","            if word in word_frequencies:\n","                word_frequencies[word] += 1\n","            else:\n","                word_frequencies[word] = 1\n","\n","        # Normalize word frequencies\n","        max_frequency = max(word_frequencies.values())\n","        for word in word_frequencies:\n","            word_frequencies[word] /= max_frequency\n","\n","        # Score sentences based on word frequencies\n","        sentence_scores = {}\n","        for sentence in sentences:\n","            sentence_words = preprocess_text(sentence)\n","            for word in sentence_words:\n","                if word in word_frequencies:\n","                    if len(sentence.split(' ')) >= min_sentence_word_count:\n","                        if sentence in sentence_scores:\n","                            sentence_scores[sentence] += word_frequencies[word]\n","                        else:\n","                            sentence_scores[sentence] = word_frequencies[word]\n","\n","        # Sort sentences by score\n","        sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n","\n","        # Select sentences until target word count is met, ensuring min_sentence_word_count\n","        summary_sentences = []\n","        word_count = 0\n","        for sentence in sorted_sentences:\n","            sentence_word_count = len(word_tokenize(sentence))\n","            if word_count + sentence_word_count <= target_word_count:\n","                summary_sentences.append(sentence)\n","                word_count += sentence_word_count\n","            elif not summary_sentences and sentence_word_count >= min_sentence_word_count:\n","                # If no sentences have been added and the least word-count sentence meets min_sentence_word_count, add it\n","                summary_sentences.append(sentence)\n","                break\n","            else:\n","                break  # Stop if we exceed the target word count\n","\n","        return \" \".join(summary_sentences)  # Return summary as a string\n","\n","    # Replace any user-specified \"\\n\" with actual line breaks\n","    text = txt.replace(\"\\\\n\", \"\\n\")\n","\n","    # Summarize the text with the target word count\n","    return summarize_text(text, target_word_count, min_sentence_word_count)\n","\n","\n","def extractive_summarization_tfidf(txt, target_word_count, min_sentence_word_count=10):\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from nltk.tokenize import sent_tokenize\n","    import numpy as np\n","    import nltk\n","    nltk.download('punkt')\n","\n","    def summarize_text(text, target_word_count, min_sentence_word_count):\n","        # Tokenize sentences\n","        sentences = sent_tokenize(text)\n","\n","        # Generate the TF-IDF matrix\n","        tfidf = TfidfVectorizer()\n","        tfidf_matrix = tfidf.fit_transform(sentences)\n","\n","        # Calculate sentence scores by summing TF-IDF values for each sentence\n","        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n","\n","        # Get indices of sentences sorted by score\n","        sorted_sentence_indices = np.argsort(sentence_scores)[::-1]\n","\n","        # Select sentences until target word count is met, respecting min_sentence_word_count\n","        summary_sentences = []\n","        word_count = 0\n","        for i in sorted_sentence_indices:\n","            sentence = sentences[i]\n","            sentence_word_count = len(sentence.split())\n","            if word_count + sentence_word_count <= target_word_count:\n","                summary_sentences.append(sentence)\n","                word_count += sentence_word_count\n","            elif not summary_sentences and sentence_word_count >= min_sentence_word_count:\n","                # If no sentences have been added and this sentence meets the minimum word count, add it\n","                summary_sentences.append(sentence)\n","                break\n","            else:\n","                break  # Stop if we exceed the target word count\n","\n","        return \" \".join(summary_sentences)  # Return summary as a string\n","\n","    # Replace any user-specified \"\\n\" with actual line breaks\n","    text = txt.replace(\"\\\\n\", \"\\n\")\n","\n","    # Summarize the text with the target word count\n","    return summarize_text(text, target_word_count, min_sentence_word_count)\n","\n","\n","def extractive_summarization_lsa(txt, target_word_count, min_sentence_word_count=10):\n","    !pip install sumy --quiet\n","    from sumy.parsers.plaintext import PlaintextParser\n","    from sumy.nlp.tokenizers import Tokenizer\n","    from sumy.summarizers.lsa import LsaSummarizer\n","\n","    def summarize_text(text, target_word_count, min_sentence_word_count):\n","        # Parse the text\n","        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n","\n","        # Initialize LSA summarizer\n","        summarizer = LsaSummarizer()\n","\n","        # Generate initial summary (retrieve all sentences scored by LSA)\n","        summary = summarizer(parser.document, len(parser.document.sentences))\n","\n","        # Sort sentences by relevance and accumulate until reaching the target word count\n","        summary_sentences = []\n","        word_count = 0\n","        for sentence in summary:\n","            sentence_text = str(sentence)\n","            sentence_word_count = len(sentence_text.split())\n","\n","            if word_count + sentence_word_count <= target_word_count:\n","                summary_sentences.append(sentence_text)\n","                word_count += sentence_word_count\n","            elif not summary_sentences and sentence_word_count >= min_sentence_word_count:\n","                # If no sentences have been added and the sentence meets min_sentence_word_count, add it\n","                summary_sentences.append(sentence_text)\n","                break\n","            else:\n","                break  # Stop if we exceed the target word count\n","\n","        return \" \".join(summary_sentences)  # Return summary as a string\n","\n","    # Replace any user-specified \"\\n\" with actual line breaks\n","    text = txt.replace(\"\\\\n\", \"\\n\")\n","\n","    # Summarize the text with the target word count\n","    return summarize_text(text, target_word_count, min_sentence_word_count)\n","\n","\n","def abstractive_summarization_bart(text, target_word_count, min_sentence_word_count=10):\n","    from transformers import pipeline\n","    import torch  # Import torch for checking GPU availability\n","\n","    # Check if GPU is available (device 0 for GPU, -1 for CPU)\n","    device = 0 if torch.cuda.is_available() else -1\n","\n","    # Initialize the BART summarization pipeline and specify the device\n","    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n","\n","    # Estimate token lengths to encourage complete sentences\n","    max_length = int(target_word_count * 2)  # Increase length to encourage full sentences\n","    min_length = max(int(target_word_count * 0.8), min_sentence_word_count)  # Ensure sentences meet min_sentence_word_count\n","\n","    # Generate the summary\n","    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n","\n","    # Extract the summary text from the output\n","    summary_text = summary[0]['summary_text']\n","\n","    # Ensure the output ends at a full stop\n","    if not summary_text.endswith('.'):\n","        last_period_index = summary_text.rfind('.')\n","        if last_period_index != -1:\n","            summary_text = summary_text[:last_period_index + 1]  # Trim to last complete sentence\n","\n","    return summary_text  # Return summary as a string\n","\n","\n","def abstractive_summarization_llm(txt, target_word_count, min_sentence_word_count=10):\n","    import os\n","    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"AAPI\")  # Replace with your actual API key\n","\n","    # Ensure the necessary libraries are installed\n","    # Commented out for execution in environments where packages are already installed\n","    !pip install --upgrade --quiet langchain langchain-google-genai beautifulsoup4\n","\n","    from langchain_google_genai import ChatGoogleGenerativeAI\n","    from langchain_core.prompts import ChatPromptTemplate\n","\n","    # Generalized function to load LLM (Gemini Models)\n","    def load_llm(model=\"gemini-1.5-pro\"):\n","        llm = ChatGoogleGenerativeAI(\n","            model=model,\n","            temperature=0,\n","            max_tokens=None,\n","            timeout=None,\n","            max_retries=2\n","        )\n","        return llm\n","\n","    # Generalized function to get a prompt template\n","    def get_prompt_template():\n","        # Define prompt\n","        prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\"system\", \"Write a concise summary of the following in {num_words} words:\\n\\n\"),\n","                (\"human\", \"{context}\")\n","            ]\n","        )\n","        return prompt\n","\n","    # Function to summarize text using Google Gemini Models\n","    def summarize_text(text, target_word_count, min_sentence_word_count, model=\"gemini-1.5-pro\"):\n","        llm = load_llm(model)\n","        prompt = get_prompt_template()\n","        chain = prompt | llm\n","\n","        result = chain.invoke({\n","            \"context\": text,\n","            \"num_words\": target_word_count\n","        })\n","\n","        # Get the generated summary\n","        summary = result.content\n","\n","        # Ensure the summary ends at a full stop and within the target word count\n","        summary_words = summary.split()\n","\n","        # If summary is longer than the target word count, trim it at the closest sentence boundary\n","        if len(summary_words) > target_word_count:\n","            summary = ' '.join(summary_words[:target_word_count])\n","\n","            # Find the last full stop to ensure the summary ends at a complete sentence\n","            last_period_index = summary.rfind('.')\n","            if last_period_index != -1:\n","                summary = summary[:last_period_index + 1]  # Trim to the last complete sentence\n","            else:\n","                # If no full stop found, trim to the nearest sentence boundary\n","                sentence_endings = ['.', '!', '?']\n","                for char in sentence_endings:\n","                    last_index = summary.rfind(char)\n","                    if last_index != -1:\n","                        summary = summary[:last_index + 1]\n","                        break\n","\n","        elif len(summary_words) < min_sentence_word_count:\n","            # If the summary is shorter than the minimum sentence word count, return the full summary\n","            return summary\n","\n","        return summary  # Return summary as a string\n","\n","    # Example text for summarization\n","    text = txt\n","\n","    # Generate and return the summary with target word count\n","    summary = summarize_text(text, target_word_count, min_sentence_word_count, model=\"gemini-1.5-flash\")\n","\n","    return summary  # Return summary as a string\n","\n","\n","def abstractive_summarization_t5(txt, target_word_count, min_sentence_word_count=10):\n","    from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","    # Load the pre-trained T5 model and tokenizer from Hugging Face\n","    model_name = \"t5-small\"\n","    model = T5ForConditionalGeneration.from_pretrained(model_name)\n","    tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","    def summarize_text(text, target_word_count, min_sentence_word_count):\n","        # Prepend \"summarize:\" to the input text\n","        input_text = \"summarize: \" + text\n","\n","        # Tokenize the input text\n","        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","        # Generate the summary (using beam search for improved quality)\n","        summary_ids = model.generate(inputs, max_length=150, min_length=40,\n","                                     length_penalty=2.0, num_beams=4, early_stopping=True)\n","\n","        # Decode the generated tokens into text\n","        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","        # Ensure the summary ends near the target word count\n","        summary_words = summary.split()\n","\n","        # If the summary is longer than the target word count, truncate it properly at a full stop\n","        if len(summary_words) > target_word_count:\n","            summary = ' '.join(summary_words[:target_word_count])\n","\n","            # Find the last full stop to ensure we don't cut off mid-sentence\n","            last_period_index = summary.rfind('.')\n","            if last_period_index != -1:\n","                summary = summary[:last_period_index + 1]  # Trim to the last complete sentence\n","            else:\n","                # If no full stop found, trim to the nearest sentence boundary (period, exclamation mark, or question mark)\n","                sentence_endings = ['.', '!', '?']\n","                for char in sentence_endings:\n","                    last_index = summary.rfind(char)\n","                    if last_index != -1:\n","                        summary = summary[:last_index + 1]\n","                        break\n","\n","        elif len(summary_words) < min_sentence_word_count:\n","            # If the summary is shorter than the minimum sentence word count, return the full summary\n","            return summary\n","\n","        return summary  # Return summary as a string\n","\n","    # Generate and return the summary with target word count\n","    summary = summarize_text(txt, target_word_count, min_sentence_word_count)\n","\n","    return summary  # Return summary as a string\n","\n","\n","def extractive_summarize_text(text, method, n):\n","    if method == \"LSA\":\n","        return extractive_summarization_lsa(text,n)\n","    elif method == \"TFIDF\":\n","        return extractive_summarization_tfidf(text,n)\n","    elif method == \"FREQUENCY\":\n","        return extractive_summarization_frequency(text,n)\n","    else:\n","        return \"Please select a valid summarization method.\"\n","\n","\n","def abstractive_summarize_text(text, method, n):\n","    if method == \"BART\":\n","        return abstractive_summarization_bart(text,n)\n","    elif method == \"LLM\":\n","        return abstractive_summarization_llm(text,n)\n","    elif method == \"T5\":\n","        return abstractive_summarization_t5(text,n)\n","    else:\n","        return \"Please select a valid summarization method.\"\n","\n","\n","\n","import gradio as gr\n","\n","css = \"\"\"\n","h1 {\n","    margin-top: 2rem;\n","    font-size: 2rem;\n","    text-align: center;\n","}\n","\"\"\"\n","\n","input_text = gr.Text(label=\"Input Text\", lines=10)\n","word_count_input = gr.Number(value=50, label=\"Number of Words for Summary\", precision=0)\n","\n","with gr.Blocks(title=\"Summarizer App\", css=css) as demo:\n","    gr.Markdown(\"# Summarizer App\")\n","\n","    with gr.Tabs():\n","        with gr.TabItem(\"Extractive\"):\n","            gr.Interface(\n","                fn=extractive_summarize_text,\n","                inputs=[\n","                    input_text,\n","                    gr.Dropdown(choices=[\"LSA\", \"FREQUENCY\", \"TFIDF\"], label=\"Select Method\"),\n","                    word_count_input  # Added input for word count\n","                ],\n","                outputs=['text'],\n","                flagging_mode='never',\n","                submit_btn='Generate'\n","            )\n","        with gr.TabItem(\"Abstractive\"):\n","            gr.Interface(\n","                fn=abstractive_summarize_text,\n","                inputs=[\n","                    input_text,\n","                    gr.Dropdown(choices=[ \"BART\", \"LLM\", \"T5\"], label=\"Select Method\"),\n","                    word_count_input  # Added input for word count\n","                ],\n","                outputs=['text'],\n","                flagging_mode='never',\n","                submit_btn='Generate'\n","            )\n","\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":889},"id":"yn7qDq35oW2T","executionInfo":{"status":"ok","timestamp":1731420017272,"user_tz":-330,"elapsed":61604,"user":{"displayName":"Akhil Kunisetty","userId":"10824639933241965364"}},"outputId":"ae43c359-fcea-4360-9705-cd3de8eca121"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d78d9c4b8a2bedc92a.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d78d9c4b8a2bedc92a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":1}]}]}