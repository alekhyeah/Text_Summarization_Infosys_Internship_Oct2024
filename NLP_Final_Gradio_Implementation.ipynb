{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Gradio Implementation"
      ],
      "metadata": {
        "id": "mFe-wBhTh3VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio and dependencies installation\n",
        "!pip install gradio --quiet\n",
        "!pip install sumy --quiet\n",
        "!pip install langchain_google_genai --quiet\n",
        "!pip install pypdf --quiet\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "1S4rJDvUnZGv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCA1pI04nb9g",
        "outputId": "694706ab-8a9d-4e18-d92a-fc66bf1af631"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency-based summarization\n",
        "def frequency_based_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        words = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_scores = {}\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in word_freq:\n",
        "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_freq[word]\n",
        "\n",
        "        sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "\n",
        "        for sentence in sorted_sentences:\n",
        "            word_count += len(word_tokenize(sentence))\n",
        "            if word_count <= max_words:\n",
        "                summary.append(sentence)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LexRank summarization\n",
        "def lexrank_summary(text, max_words):\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        summarizer = LexRankSummarizer()\n",
        "        sentences = parser.document.sentences\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for sentence in summarizer(parser.document, len(sentences)):\n",
        "            sentence_words = len(word_tokenize(str(sentence)))\n",
        "            if word_count + sentence_words <= int(max_words):\n",
        "                summary.append(str(sentence))\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# TextRank summarization\n",
        "def textrank_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        clean_sentences = []\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        for sentence in sentences:\n",
        "            words = word_tokenize(sentence.lower())\n",
        "            words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "            clean_sentences.append(\" \".join(words))\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(graph)\n",
        "        ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for _, sentence in ranked_sentences:\n",
        "            sentence_words = len(word_tokenize(sentence))\n",
        "            if word_count + sentence_words <= max_words:\n",
        "                summary.append(sentence)\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# T5 summarization\n",
        "def t5_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "        input_text = \"summarize: \" + text\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# BART summarization\n",
        "def bart_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "        inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LLM (Gemini) summarization\n",
        "def llm_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, max_output_tokens=max_words)\n",
        "        prompt = ChatPromptTemplate.from_messages([(\"system\", f\"Summarize this text in {max_words} words:\\n\\n\"), (\"human\", text)])\n",
        "        chain = prompt | llm\n",
        "        result = chain.invoke({\"text\": text})\n",
        "        return result.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Iterative Refinement without requiring an existing summary\n",
        "def iterative_refinement_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, max_output_tokens=max_words)\n",
        "\n",
        "        # Split text into manageable chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        docs = text_splitter.create_documents([text])\n",
        "\n",
        "        # Refinement chain\n",
        "        prompt = PromptTemplate(template=\"Write a concise summary of the following text:\\n\\n{text}\\n\\nSUMMARY:\", input_variables=[\"text\"])\n",
        "        summary_chain = load_summarize_chain(\n",
        "            llm,\n",
        "            chain_type=\"map_reduce\",\n",
        "            map_prompt=prompt,\n",
        "            combine_prompt=prompt\n",
        "        )\n",
        "\n",
        "        # Generate the summary\n",
        "        summary = summary_chain.run(docs)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error in Iterative Refinement Summary: {str(e)}\"\n",
        "\n",
        "# PDF Summarization\n",
        "def pdf_summarizer(pdf_file, summarization_type, algorithm, max_words):\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_file)\n",
        "        text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "\n",
        "        if summarization_type == \"Extractive\":\n",
        "            return extractive_summarize(text, algorithm, max_words)\n",
        "        elif summarization_type == \"Abstractive\":\n",
        "            return abstractive_summarize(text, algorithm, max_words)\n",
        "        elif summarization_type == \"LLM\":\n",
        "            return llm_summarize(text, algorithm, max_words)\n",
        "        else:\n",
        "            return \"Invalid summarization type.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in PDF summarization: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Text Summarization App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Summarization\"):\n",
        "            gr.Markdown(\"## Select Summarization Type and Method\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    input_type = gr.Radio(\n",
        "                        [\"Text Input\", \"PDF Upload\"],\n",
        "                        label=\"Input Type\",\n",
        "                        value=\"Text Input\"\n",
        "                    )\n",
        "\n",
        "                    # Text Input\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"Input Text\",\n",
        "                        lines=5,\n",
        "                        placeholder=\"Enter text here\",\n",
        "                        visible=True\n",
        "                    )\n",
        "\n",
        "                    # PDF Upload\n",
        "                    pdf_input = gr.File(\n",
        "                        label=\"Upload PDF\",\n",
        "                        type=\"filepath\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        visible=False\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    summarization_type = gr.Radio(\n",
        "                        [\"Extractive\", \"Abstractive\", \"LLM\"],\n",
        "                        label=\"Summarization Type\",\n",
        "                        value=\"Extractive\"\n",
        "                    )\n",
        "\n",
        "                    method_dropdown = gr.Dropdown(\n",
        "                        label=\"Algorithm\",\n",
        "                        visible=True\n",
        "                    )\n",
        "\n",
        "                    words = gr.Textbox(\n",
        "                        label=\"Maximum Words\",\n",
        "                        value=\"100\"\n",
        "                    )\n",
        "\n",
        "            # Output\n",
        "            output = gr.Textbox(\n",
        "                label=\"Summary\",\n",
        "                lines=5\n",
        "            )\n",
        "\n",
        "            # Generate Button\n",
        "            button = gr.Button(\"Generate Summary\")\n",
        "\n",
        "            # Input Type Toggle\n",
        "            def toggle_inputs(choice):\n",
        "                if choice == \"Text Input\":\n",
        "                    return {\n",
        "                        text_input: gr.update(visible=True),\n",
        "                        pdf_input: gr.update(visible=False)\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        text_input: gr.update(visible=False),\n",
        "                        pdf_input: gr.update(visible=True)\n",
        "                    }\n",
        "\n",
        "            input_type.change(\n",
        "                toggle_inputs,\n",
        "                inputs=input_type,\n",
        "                outputs=[text_input, pdf_input]\n",
        "            )\n",
        "\n",
        "            # Update Algorithms Dropdown\n",
        "            def update_algorithms(summarization_type):\n",
        "                if summarization_type == \"Extractive\":\n",
        "                    return gr.update(choices=[\"Frequency-based\", \"LexRank\", \"TextRank\"], visible=True)\n",
        "                elif summarization_type == \"Abstractive\":\n",
        "                    return gr.update(choices=[\"T5\", \"BART\"], visible=True)\n",
        "                elif summarization_type == \"LLM\":\n",
        "                    return gr.update(choices=[\"LLM (Gemini)\", \"Map Reduce\", \"Iterative Refinement\"], visible=True)\n",
        "                else:\n",
        "                    return gr.update(visible=False)\n",
        "\n",
        "            summarization_type.change(\n",
        "                update_algorithms,\n",
        "                inputs=summarization_type,\n",
        "                outputs=method_dropdown\n",
        "            )\n",
        "\n",
        "            # Summarization Logic\n",
        "            def summarize(input_type, text, pdf, summarization_type, method, max_words):\n",
        "                if input_type == \"Text Input\":\n",
        "                    if summarization_type == \"Extractive\":\n",
        "                        return extractive_summarize(text, method, max_words)\n",
        "                    elif summarization_type == \"Abstractive\":\n",
        "                        return abstractive_summarize(text, method, max_words)\n",
        "                    elif summarization_type == \"LLM\":\n",
        "                        return llm_summarize(text, method, max_words)\n",
        "                    else:\n",
        "                        return \"Invalid summarization type.\"\n",
        "                else:  # PDF Upload\n",
        "                    return pdf_summarizer(pdf, summarization_type, method, max_words)\n",
        "\n",
        "            button.click(\n",
        "                summarize,\n",
        "                inputs=[input_type, text_input, pdf_input, summarization_type, method_dropdown, words],\n",
        "                outputs=output\n",
        "            )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "yUXKpIZiaK0x",
        "outputId": "075b80b7-15f5-4237-c996-d90d4b4eea73"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e01907e2bffaa8da66.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e01907e2bffaa8da66.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MrKc3Q5iMHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}