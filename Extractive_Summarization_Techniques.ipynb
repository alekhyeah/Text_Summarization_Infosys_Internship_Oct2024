{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Frequency Method"
      ],
      "metadata": {
        "id": "X2G_MzFUpX3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jekXVUAur0y9",
        "outputId": "5201b82f-25f7-45e8-fb49-e40b05434a15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    # Preprocessing\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Word frequency calculation\n",
        "    word_frequencies = defaultdict(int)\n",
        "    for word in word_tokenize(text):\n",
        "        if word not in stop_words and word not in string.punctuation:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "    # Score sentences\n",
        "    sentence_scores = defaultdict(int)\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "    # Select top N sentences\n",
        "    summarized_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "\n",
        "    return ' '.join(summarized_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFShWSeSr5ik",
        "outputId": "43df53a6-dfe5-482d-a0d3-dd3b2cde72e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. With the growth of data and information, NLP is becoming more crucial in analyzing text data and extracting insights.\"\"\"\n",
        "\n",
        "summary = extractive_summary(text, num_sentences=2)\n",
        "print(\"Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t81TrUNyr8CS",
        "outputId": "a098620a-cb84-4414-9ee0-b7aa3a465d03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "natural language processing (nlp) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. with the growth of data and information, nlp is becoming more crucial in analyzing text data and extracting insights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.TextRank Method\n"
      ],
      "metadata": {
        "id": "jvtHpcHgsdON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1E-GE72sg4m",
        "outputId": "933e230f-cc74-4205-84d8-9939f891c813"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.26.4)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54387 sha256=6c110120e84b6267080193937bd209a5c72a4d9f0488801db65c13372ba45e57\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import summarizer"
      ],
      "metadata": {
        "id": "MGom7rwetACg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
        "\n",
        "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
        "\n",
        "The importance of AI is growing in various fields, including healthcare, finance, and transportation. By leveraging vast amounts of data, AI systems can identify patterns, make decisions, and improve outcomes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uIJRDrfytAce"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank_summary(text, ratio=0.2):\n",
        "    \"\"\"\n",
        "    Summarizes the given text using the TextRank algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to summarize.\n",
        "    - ratio (float): The ratio of sentences to include in the summary.\n",
        "\n",
        "    Returns:\n",
        "    - str: The summarized text.\n",
        "    \"\"\"\n",
        "    return summarizer.summarize(text, ratio=ratio)"
      ],
      "metadata": {
        "id": "zoVsfrEGtDG4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = textrank_summary(text, ratio=0.3)  # Adjust the ratio as needed\n",
        "print(\"TextRank Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccmPBhOotGCK",
        "outputId": "65cff3bd-0eb8-4c9a-b0af-0aac55fbd836"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextRank Summary:\n",
            "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Luhn Method"
      ],
      "metadata": {
        "id": "2_xA6YzOtOeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLbRiLwPtQQU",
        "outputId": "7e811336-c857-4438-9d2b-effb0eb1ca74"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "metadata": {
        "id": "2dPw3A_StS-T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msdVEno8tVii",
        "outputId": "95acf85e-88d1-4747-b058-dd1d7246d19b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
        "\n",
        "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
        "\n",
        "The importance of AI is growing in various fields, including healthcare, finance, and transportation. By leveraging vast amounts of data, AI systems can identify patterns, make decisions, and improve outcomes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "grZGINbktYle"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def luhn_summary(text, num_sentences=2):\n",
        "    \"\"\"\n",
        "    Summarizes the given text using the Luhn algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to summarize.\n",
        "    - num_sentences (int): The number of sentences to include in the summary.\n",
        "\n",
        "    Returns:\n",
        "    - str: The summarized text.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)  # Split text into sentences\n",
        "    word_freq = defaultdict(int)\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        for word in words:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "    # Calculate sentence scores based on the frequency of significant words\n",
        "    sentence_scores = defaultdict(int)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        for word in words:\n",
        "            if word_freq[word] > 1:  # Only consider words that appear more than once\n",
        "                sentence_scores[i] += word_freq[word]\n",
        "\n",
        "    # Select the top N sentences based on scores\n",
        "    ranked_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected_sentences = sorted([ranked_sentences[i][0] for i in range(num_sentences)])\n",
        "\n",
        "    # Construct the summary\n",
        "    summary = ' '.join([sentences[i] for i in selected_sentences])\n",
        "    return summary"
      ],
      "metadata": {
        "id": "Cj7bIGeLtbqf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = luhn_summary(text, num_sentences=2)  # Adjust 'num_sentences' as needed\n",
        "print(\"Luhn Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNa39j27tmvg",
        "outputId": "bce04f08-bbf6-4794-e19c-479fa996ff7b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Luhn Summary:\n",
            "\n",
            "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-COMBINED CODE"
      ],
      "metadata": {
        "id": "gJsCF61ZuuNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# Frequency Method\n",
        "def frequency_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = preprocess_text(text)\n",
        "    freq = nltk.FreqDist(words)\n",
        "    ranking = {}\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for word in preprocess_text(sentence):\n",
        "            if word in freq:\n",
        "                if i in ranking:\n",
        "                    ranking[i] += freq[word]\n",
        "                else:\n",
        "                    ranking[i] = freq[word]\n",
        "\n",
        "    ranked_sentences = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected_sentences = sorted([ranked_sentences[i][0] for i in range(num_sentences)])\n",
        "    return [sentences[j] for j in selected_sentences]\n",
        "\n",
        "# TextRank Method\n",
        "def textrank_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # TextRank Algorithm\n",
        "    n = len(sentences)\n",
        "    scores = np.ones(n)\n",
        "    for _ in range(10):  # Iterative update\n",
        "        scores = (1 - 0.85) + 0.85 * cosine_matrix.dot(scores) / np.sum(cosine_matrix, axis=1)\n",
        "\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    return [ranked_sentences[i][1] for i in range(num_sentences)]\n",
        "\n",
        "# Luhn Method\n",
        "def luhn_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = preprocess_text(text)\n",
        "    freq = nltk.FreqDist(words)\n",
        "    ranking = {}\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_words = preprocess_text(sentence)\n",
        "        score = 0\n",
        "        for word in sentence_words:\n",
        "            if word in freq:\n",
        "                score += freq[word]\n",
        "                if freq[word] > 1:\n",
        "                    score += (freq[word] - 1) * 0.5  # Weight for frequency\n",
        "        ranking[i] = score\n",
        "\n",
        "    ranked_sentences = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected_sentences = sorted([ranked_sentences[i][0] for i in range(num_sentences)])\n",
        "    return [sentences[j] for j in selected_sentences]\n",
        "\n",
        "# Example usage\n",
        "text = \"\"\"\n",
        "Hard work is the act of putting in time and effort to achieve goals, overcome obstacles, and reach success. It involves physical and mental effort, and requires dedication and persistence.To achieve your dreams, you need to work hard and be determined and focused on your path. A man is like steel; if you use it, then it shines; and if you let it rest, then it rusts. If you have truly worked hard, then it will definitely pay off, and you will definitely enjoy your success.Hard work can lead to personal development and achievements, and can help you become more disciplined, focused, and confident. It can also help you earn respect, and make you more determined and responsible. Hard work is essential for success in life, and is a necessary component of our lives. It can help you make the best use of your time, and prevent you from wasting time on trivial things.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Frequency Method Summary:\")\n",
        "print(frequency_method(text))\n",
        "\n",
        "print(\"\\nTextRank Method Summary:\")\n",
        "print(textrank_method(text))\n",
        "\n",
        "print(\"\\nLuhn Method Summary:\")\n",
        "print(luhn_method(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99C0tv55vF2v",
        "outputId": "fcc6f13b-7dd6-45d8-d7b0-be5f1ab94e8e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Frequency Method Summary:\n",
            "['It involves physical and mental effort, and requires dedication and persistence.To achieve your dreams, you need to work hard and be determined and focused on your path.', 'A man is like steel; if you use it, then it shines; and if you let it rest, then it rusts.', 'If you have truly worked hard, then it will definitely pay off, and you will definitely enjoy your success.Hard work can lead to personal development and achievements, and can help you become more disciplined, focused, and confident.']\n",
            "\n",
            "TextRank Method Summary:\n",
            "['It involves physical and mental effort, and requires dedication and persistence.To achieve your dreams, you need to work hard and be determined and focused on your path.', 'It can help you make the best use of your time, and prevent you from wasting time on trivial things.', 'It can also help you earn respect, and make you more determined and responsible.']\n",
            "\n",
            "Luhn Method Summary:\n",
            "['It involves physical and mental effort, and requires dedication and persistence.To achieve your dreams, you need to work hard and be determined and focused on your path.', 'A man is like steel; if you use it, then it shines; and if you let it rest, then it rusts.', 'If you have truly worked hard, then it will definitely pay off, and you will definitely enjoy your success.Hard work can lead to personal development and achievements, and can help you become more disciplined, focused, and confident.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Route Score"
      ],
      "metadata": {
        "id": "UTt7ZfEwy2aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# Frequency Method\n",
        "def frequency_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = preprocess_text(text)\n",
        "    freq = nltk.FreqDist(words)\n",
        "    ranking = {}\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for word in preprocess_text(sentence):\n",
        "            if word in freq:\n",
        "                if i in ranking:\n",
        "                    ranking[i] += freq[word]\n",
        "                else:\n",
        "                    ranking[i] = freq[word]\n",
        "\n",
        "    ranked_sentences = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    num_sentences = min(num_sentences, len(sentences))\n",
        "    selected_sentences = sorted([ranked_sentences[i][0] for i in range(num_sentences)])\n",
        "    return [sentences[j] for j in selected_sentences]\n",
        "\n",
        "# TextRank Method\n",
        "def textrank_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    n = len(sentences)\n",
        "    scores = np.ones(n)\n",
        "    for _ in range(10):  # Iterative update\n",
        "        scores = (1 - 0.85) + 0.85 * cosine_matrix.dot(scores) / np.sum(cosine_matrix, axis=1)\n",
        "\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    num_sentences = min(num_sentences, len(sentences))\n",
        "    return [ranked_sentences[i][1] for i in range(num_sentences)]\n",
        "\n",
        "# Luhn Method\n",
        "def luhn_method(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = preprocess_text(text)\n",
        "    freq = nltk.FreqDist(words)\n",
        "    ranking = {}\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_words = preprocess_text(sentence)\n",
        "        score = 0\n",
        "        for word in sentence_words:\n",
        "            if word in freq:\n",
        "                score += freq[word]\n",
        "                if freq[word] > 1:\n",
        "                    score += (freq[word] - 1) * 0.5  # Weight for frequency\n",
        "        ranking[i] = score\n",
        "\n",
        "    ranked_sentences = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    num_sentences = min(num_sentences, len(sentences))\n",
        "    selected_sentences = sorted([ranked_sentences[i][0] for i in range(num_sentences)])\n",
        "    return [sentences[j] for j in selected_sentences]\n",
        "\n",
        "# Function to compute ROUGE scores\n",
        "def compute_rouge(reference_summary, generated_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(\" \".join(reference_summary), \" \".join(generated_summary), avg=True)\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "reference_summary = [\n",
        "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"\n",
        "    \"Colloquially, the term artificial intelligence is often used to describe machines (or computers) that mimic functions that humans associate with the human mind.\"\n",
        "    \"The importance of AI is growing in various fields, including healthcare, finance, and transportation. By leveraging vast amounts of data, AI systems can identify patterns, make decisions, and improve outcomes.\"]\n",
        "\n",
        "text = \"\"\"\n",
        "Hard work is the act of putting in time and effort to achieve goals, overcome obstacles, and reach success. It involves physical and mental effort, and requires dedication and persistence.To achieve your dreams, you need to work hard and be determined and focused on your path. A man is like steel; if you use it, then it shines; and if you let it rest, then it rusts. If you have truly worked hard, then it will definitely pay off, and you will definitely enjoy your success.Hard work can lead to personal development and achievements, and can help you become more disciplined, focused, and confident. It can also help you earn respect, and make you more determined and responsible. Hard work is essential for success in life, and is a necessary component of our lives. It can help you make the best use of your time, and prevent you from wasting time on trivial things.\n",
        "\"\"\"\n",
        "\n",
        "# Generate summaries\n",
        "frequency_summary = frequency_method(text)\n",
        "textrank_summary = textrank_method(text)\n",
        "luhn_summary = luhn_method(text)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "frequency_rouge = compute_rouge(reference_summary, frequency_summary)\n",
        "textrank_rouge = compute_rouge(reference_summary, textrank_summary)\n",
        "luhn_rouge = compute_rouge(reference_summary, luhn_summary)\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(\"Frequency Method ROUGE Scores:\")\n",
        "print(frequency_rouge)\n",
        "\n",
        "print(\"\\nTextRank Method ROUGE Scores:\")\n",
        "print(textrank_rouge)\n",
        "\n",
        "print(\"\\nLuhn Method ROUGE Scores:\")\n",
        "print(luhn_rouge)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGdJX8UIy5QD",
        "outputId": "b6e28593-b89f-47cf-d276-b0e63573e8b2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Frequency Method ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.07142857142857142, 'f': 0.06896551224732497}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.07142857142857142, 'f': 0.06896551224732497}}\n",
            "\n",
            "TextRank Method ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.10714285714285714, 'f': 0.12244897469387774}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.10714285714285714, 'f': 0.12244897469387774}}\n",
            "\n",
            "Luhn Method ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.07142857142857142, 'f': 0.06896551224732497}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.07142857142857142, 'f': 0.06896551224732497}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}