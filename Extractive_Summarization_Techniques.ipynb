{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Text summarization using the frequency method\n"
      ],
      "metadata": {
        "id": "3bnCfYezdO9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Importing Libraries\n"
      ],
      "metadata": {
        "id": "7XUot6PXdbfv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GABtSxJOdBth"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Generate a list of stopwords in English"
      ],
      "metadata": {
        "id": "pB9jULQDdrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords1 = set(stopwords.words(\"english\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoGxPwvZdrha",
        "outputId": "d4c4fef2-9a13-48ad-e0c7-650f79b2ef09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Tokenize the Words"
      ],
      "metadata": {
        "id": "N66X6Z6neAGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "text = \"my name is shubham kumar shukla. It is my pleasure to got opportunity to write article for xyz related to nlp\"\n",
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUDXtrzveESd",
        "outputId": "f8813420-3a4f-4bd9-dbd1-4d1a73a9d0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Create Frequency Table\n",
        "\n",
        "For each word in words:\n",
        "Convert the word to lowercase.\n",
        "If the word is a stopword, skip it.\n",
        "If the word is already in the frequency table, increment its count.\n",
        "Otherwise, add the word to the frequency table with an initial count of 1."
      ],
      "metadata": {
        "id": "yfuEwH8seG4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqTable = {}\n",
        "for word in words:\n",
        "  word = word.lower()\n",
        "  if word in stopwords1:\n",
        "    continue\n",
        "  if word in freqTable:\n",
        "    freqTable[word] += 1\n",
        "  else:\n",
        "    freqTable[word] = 1\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i_7G_ctneKWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 Tokenize the Sentences"
      ],
      "metadata": {
        "id": "SfuF34vXeMZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "sentenceValue = {}"
      ],
      "metadata": {
        "id": "Qqe9m2dfedAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.6 Assign Scores to Sentences"
      ],
      "metadata": {
        "id": "LgXoA4snegrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  for word, freq in freqTable.items():\n",
        "    if word in sentence.lower():\n",
        "      if sentence in sentenceValue:\n",
        "        sentenceValue[sentence] += freq\n",
        "      else:\n",
        "        sentenceValue[sentence] = freq\n"
      ],
      "metadata": {
        "id": "iZvRa1kiekct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.7 Calculate the Average Sentence Score"
      ],
      "metadata": {
        "id": "QM6DOC0Qepah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sumValues = 0\n",
        "for sentence in sentenceValue:\n",
        "  sumValues += sentenceValue[sentence]\n",
        "average = int(sumValues / len(sentenceValue))\n"
      ],
      "metadata": {
        "id": "g0btdUH1epph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.8 Generate the Summary"
      ],
      "metadata": {
        "id": "hH6-QdKWeyRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ''\n",
        "for sentence in sentences:\n",
        "  if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "    summary += \"\" + sentence\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vesiADNpeyfo",
        "outputId": "98bf3614-8c9d-4af5-bd73-e8ad5f69a1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is my pleasure to got opportunity to write article for xyz related to nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sumy: Sumy is a textrank based machine learning algorithm."
      ],
      "metadata": {
        "id": "Esc-0ek5gbi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Install Sumy"
      ],
      "metadata": {
        "id": "Z12Gf5yyglgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAxmEfP4g1Vq",
        "outputId": "20cdac61-e28c-4cf1-be15-fd591f168c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=043cbd4e25f47409cb3b4bfd675b6697d4d3302e1017faff5a9d280cd5330a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=ee39d56b1e95bd1c58b33e27ccbdeaae9c66db7c71b742fb91ddbb3671b6b0b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Import packages"
      ],
      "metadata": {
        "id": "ZVHfnYG1g-c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer"
      ],
      "metadata": {
        "id": "VZivJYR_ghMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Creating text parser using tokenization"
      ],
      "metadata": {
        "id": "XZmMDNbHhIJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))"
      ],
      "metadata": {
        "id": "aSHdT5hAhZ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 Summarize using sumy TextRank"
      ],
      "metadata": {
        "id": "DhPuEk4yhOPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the number of sentences in summary: \"))\n",
        "summarizer = TextRankSummarizer()\n",
        "summary = summarizer(parser.document, n)"
      ],
      "metadata": {
        "id": "l4Pz1gYCgvaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5 Generate summary"
      ],
      "metadata": {
        "id": "zTi97kjchkly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_summary = \"\"\n",
        "for sentence in summary:\n",
        "  text_summary += str(sentence)\n",
        "\n",
        "print(text_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu87ueqjhgsQ",
        "outputId": "db9d9b11-f7c4-4ee9-fbfe-94114212ee7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is my pleasure to got opportunity to write article for xyz related to nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Lex Rank:\n",
        "\n",
        "  This is an unsupervised machine learning based approach in which we use the textrank approach to find the summary of our sentences. Using cosine similarity and vector based algorithms, we find minimum cosine distance among various words and store the more similar words together."
      ],
      "metadata": {
        "id": "DpElrdeXhwNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Import packages"
      ],
      "metadata": {
        "id": "lfIFVk-Hh6IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ],
      "metadata": {
        "id": "WdGQLQ9ah9_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Create Summarizer"
      ],
      "metadata": {
        "id": "xDnmb4JAis2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LexRankSummarizer()"
      ],
      "metadata": {
        "id": "VDLd9uEuho55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Generate Summary"
      ],
      "metadata": {
        "id": "euK7S9ROi-mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(parser.document, 1)"
      ],
      "metadata": {
        "id": "rY-8a5nWiLxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Print Summary"
      ],
      "metadata": {
        "id": "GTDnfaKOjBn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp = []\n",
        "for i in summary:\n",
        "  lp = str(i)\n",
        "  dp.append(lp)\n",
        "final_sentence = ' '.join(dp)\n",
        "print(final_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7QwIOoxiSLu",
        "outputId": "6996a67a-01f2-4231-9e0e-5312dac27e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is shubham kumar shukla.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. LSA\n",
        "\n",
        "  Latent Semantic Analyzer (LSA) is based on decomposing the data into low dimensional space. LSA has the ability to store the semantic of given text while summarizing."
      ],
      "metadata": {
        "id": "SeNjLfIijE5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Import LSA package"
      ],
      "metadata": {
        "id": "0XLld6FGjxme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.summarizers.lsa import LsaSummarizer"
      ],
      "metadata": {
        "id": "Fco3WZE1ijGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Create Summarizer"
      ],
      "metadata": {
        "id": "DUAVhcOIj1nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the number of sentences in summary: \"))\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer_lsa = LsaSummarizer()\n",
        "summary_2 = summarizer_lsa(parser.document, n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a__GdXqjNtS",
        "outputId": "06f369af-a183-4b80-ca63-f037a44c695f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of sentences in summary: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3 Generate Summary and print it"
      ],
      "metadata": {
        "id": "nHaQTA8-j9Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp = []\n",
        "for i in summary_2:\n",
        "  lp = str(i)\n",
        "dp.append(lp)\n",
        "final_sentence = ' '.join(dp)\n",
        "print(final_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0icQepoHjSe9",
        "outputId": "1d163f75-b3f3-43d6-fcaf-3bd4d700e9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is shubham kumar shukla.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Combined Code"
      ],
      "metadata": {
        "id": "3MYObTeLtMOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_method(text):\n",
        "  nltk.download('stopwords')\n",
        "  stopwords1 = set(stopwords.words(\"english\"))\n",
        "  nltk.download('punkt')\n",
        "  words = word_tokenize(text)\n",
        "\n",
        "  #Frequency Table Creation\n",
        "  freqTable = {}\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in stopwords1:\n",
        "      continue\n",
        "    if word in freqTable:\n",
        "      freqTable[word] += 1\n",
        "    else:\n",
        "      freqTable[word] = 1\n",
        "\n",
        "  sentences = sent_tokenize(text)\n",
        "  sentenceValue = {}\n",
        "\n",
        "\n",
        "  for sentence in sentences:\n",
        "    for word, freq in freqTable.items():\n",
        "      if word in sentence.lower():\n",
        "        if sentence in sentenceValue:\n",
        "          sentenceValue[sentence] += freq\n",
        "        else:\n",
        "          sentenceValue[sentence] = freq\n",
        "\n",
        "  sumValues = 0\n",
        "  for sentence in sentenceValue:\n",
        "    sumValues += sentenceValue[sentence]\n",
        "  average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "  summary = ''\n",
        "  for sentence in sentences:\n",
        "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "      summary += \"\" + sentence\n",
        "\n",
        "  return(summary)\n",
        "\n",
        "\n",
        "def sumy(text,n):\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "  summarizer = TextRankSummarizer()\n",
        "  summary = summarizer(parser.document, n)\n",
        "\n",
        "  text_summary = \"\"\n",
        "  for sentence in summary:\n",
        "    text_summary += str(sentence)\n",
        "    text_summary+='\\n'\n",
        "\n",
        "  return(text_summary)\n",
        "\n",
        "\n",
        "def lex_rank(text,n):\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "  summarizer = LexRankSummarizer()\n",
        "\n",
        "  summary = summarizer(parser.document, n)\n",
        "  dp = []\n",
        "  for i in summary:\n",
        "    lp = str(i)\n",
        "    dp.append(lp)\n",
        "  final_sentence = '\\n'.join(dp)\n",
        "  return(final_sentence)\n",
        "\n",
        "def lsa(text,n):\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "  summarizer_lsa = LsaSummarizer()\n",
        "  summary_2 = summarizer_lsa(parser.document, n)\n",
        "\n",
        "  dp = []\n",
        "  for i in summary_2:\n",
        "    lp = str(i)\n",
        "  dp.append(lp)\n",
        "  final_sentence = '\\n'.join(dp)\n",
        "  return(final_sentence)\n",
        "\n",
        "\n",
        "\n",
        "algo = int(input('''Please choose the algorithm you wish to use for summarizing text\n",
        "1. Frequency Method\n",
        "2. Sumy\n",
        "3. Lex Rank\n",
        "4. LSA\n",
        "\n",
        "'''))\n",
        "\n",
        "text = input('Enter Input Text: ')\n",
        "\n",
        "\n",
        "if(algo==1):\n",
        "  print('\\nText Summary: \\n')\n",
        "  print(freq_method(text))\n",
        "elif(algo==2):\n",
        "  n = int(input(\"Enter number of lines in summary\"))\n",
        "  print('\\nText Summary: \\n')\n",
        "  print(sumy(text,n))\n",
        "elif(algo==3):\n",
        "  n = int(input(\"Enter number of lines in summary\"))\n",
        "  print('\\nText Summary: \\n')\n",
        "  print(lex_rank(text,n))\n",
        "elif(algo==4):\n",
        "  n = int(input(\"Enter number of lines in summary\"))\n",
        "  print('\\nText Summary: \\n')\n",
        "  print(lsa(text,n))\n",
        "else:\n",
        "  print(\"Invalid Input\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqLuB6oEjXGj",
        "outputId": "42d8702a-1d0a-4287-9620-b62cd07f6566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please choose the algorithm you wish to use for summarizing text\n",
            "1. Frequency Method\n",
            "2. Sumy\n",
            "3. Lex Rank\n",
            "4. LSA\n",
            "\n",
            "3\n",
            "Enter Input Text: Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n",
            "Enter number of lines in summary3\n",
            "\n",
            "Text Summary: \n",
            "\n",
            "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
            "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
            "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQBxYA1-ucLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}