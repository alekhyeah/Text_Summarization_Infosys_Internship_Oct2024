# Text_Summarization_Infosys_Internship_Oct2024
This Text Summarization Project aims to develop a tool that efficiently condenses long articles, papers, or documents into concise summaries while preserving the key information and context. Utilizing advanced Natural Language Processing (NLP) techniques, this project focuses on both extractive and abstractive summarization methods.

# Week 1 - Extractive Summarization

Frequency Method: Summarization using term frequency to rank and select the most relevant sentences.
Sumy Library: Utilized an established library for extractive summarization, offering more flexibility and prebuilt algorithms.

# Week 2 - Abstractive Summarization

T5 Model: Implemented Google's T5 transformer model to generate human-like summaries.
BART Model: Used the BART transformer model for its ability to paraphrase and compress text effectively.

# Week 3 & 4 - Evaluation Metrics: ROUGE and BLEU Scores

Calculated ROUGE scores to measure overlap between the generated and reference summaries.
Computed BLEU scores to evaluate the quality of machine-generated summaries.

# Week 5 - LLM-based Summarization

Implemented Large Language Models (LLMs) for summarization

# Week 6 - LangChain Summarization

Leveraged LangChain to process PDF documents and create concise summaries.

# Week 7 & 8 - Gradio Interface Development

Combined extractive, abstractive, and LLM-based summarization methods into a single tool.
Added features such as PDF upload, text input, output word limit setting, and summary evaluation.
