{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "i-VpqebbkwbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "id": "vO5bALC1lD5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain[google]"
      ],
      "metadata": {
        "id": "Z2ePtQ4jmqKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] =userdata.get('GOOGLE_API_KEY')\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')"
      ],
      "metadata": {
        "id": "A8dPCI2Rm1qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain langchain-google-genai langchain-huggingface"
      ],
      "metadata": {
        "id": "kOeiX0cNm98Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d48b978-f5af-4390-e931-3c02779900bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.0/125.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "QET3z7vMm60c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCp0y-mLkY7e"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, BartForConditionalGeneration, BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download nltk data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "QNmnUGVUlOHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for Extractive Summarization Methods\n",
        "def frequency_summarize(text, max_words):\n",
        "    stopwords1 = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "    freqTable = {word: words.count(word) for word in set(words) if word.lower() not in stopwords1}\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = {}\n",
        "    for sentence in sentences:\n",
        "        sentenceValue[sentence] = sum(freqTable.get(word.lower(), 0) for word in word_tokenize(sentence))\n",
        "    sorted_sentences = sorted(sentenceValue.items(), key=lambda item: item[1], reverse=True)\n",
        "    summary = ' '.join(sentence for sentence, score in sorted_sentences[:3])\n",
        "    if len(summary.split()) > max_words:\n",
        "        summary = ' '.join(summary.split()[:max_words])\n",
        "    return summary\n",
        "\n",
        "def lsa_summarize(text, max_words):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary = ' '.join(str(sentence) for sentence in summarizer(parser.document, 3))\n",
        "    return summary if len(summary.split()) <= max_words else ' '.join(summary.split()[:max_words])\n",
        "\n",
        "def lexrank_summarize(text, max_words):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = ' '.join(str(sentence) for sentence in summarizer(parser.document, 3))\n",
        "    return summary if len(summary.split()) <= max_words else ' '.join(summary.split()[:max_words])\n",
        "\n",
        "def luhn_summarize(text, max_words):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    summary = ' '.join(str(sentence) for sentence in summarizer(parser.document, 3))\n",
        "    return summary if len(summary.split()) <= max_words else ' '.join(summary.split()[:max_words])\n"
      ],
      "metadata": {
        "id": "OZmBeqwyllB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Abstractive Summarization Methods\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def t5_summarize(text, max_words):\n",
        "    inputs = t5_tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = t5_model.generate(inputs, max_length=max_words, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def bart_summarize(text, max_words):\n",
        "    inputs = bart_tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = bart_model.generate(inputs, max_length=max_words, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# LLM Summarizer\n",
        "def load_llm(model=\"gemini-1.5-pro\"):\n",
        "    return ChatGoogleGenerativeAI(model=model, temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
        "\n",
        "def get_prompt_template():\n",
        "    return ChatPromptTemplate.from_messages([(\"system\", \"Summarize this in {num_words} words:\\n\\n\"), (\"human\", \"{context}\")])\n",
        "\n",
        "def llm_summarize(text, model=\"gemini-1.5-pro\", num_words=50):\n",
        "    llm = load_llm(model)\n",
        "    prompt = get_prompt_template()\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"context\": text, \"num_words\": num_words})\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "EJKfR1L4oMnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Assuming the summarization functions are defined elsewhere\n",
        "# from your_summarization_module import frequency_summarize, lsa_summarize, lexrank_summarize, luhn_summarize\n",
        "# from your_summarization_module import t5_summarize, bart_summarize, llm_summarize\n",
        "\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "with gr.Blocks(title=\"Summarizer App\") as demo:\n",
        "    gr.Markdown(\"# Summarizer App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Extractive summarization tab\n",
        "        with gr.TabItem(\"Extractive Summarization\"):\n",
        "            extractive_method_choice = gr.Dropdown(choices=[\"Frequency\", \"LSA\", \"LexRank\", \"Luhn\"], label=\"Select Extractive Method\")\n",
        "            extractive_max_words = gr.Number(label=\"Max Words\", value=50)\n",
        "\n",
        "            with gr.Row():\n",
        "                # Left column for input text\n",
        "                with gr.Column(scale=1):\n",
        "                    extractive_input_text = gr.Textbox(label=\"Input Text\", lines=10, placeholder=\"Enter text to summarize\")\n",
        "\n",
        "                # Right column for output text and word counts\n",
        "                with gr.Column(scale=1):\n",
        "                    extractive_output_text = gr.Textbox(label=\"Extractive Summary\", placeholder=\"Summary will appear here\")\n",
        "                    input_word_count = gr.Textbox(label=\"Input Word Count\", value=\"0\", interactive=False)\n",
        "                    output_word_count = gr.Textbox(label=\"Output Word Count\", value=\"0\", interactive=False)\n",
        "\n",
        "            def extractive_summarize(text, method, max_words):\n",
        "                if method == \"Frequency\":\n",
        "                    summary = frequency_summarize(text, max_words)\n",
        "                elif method == \"LSA\":\n",
        "                    summary = lsa_summarize(text, max_words)\n",
        "                elif method == \"LexRank\":\n",
        "                    summary = lexrank_summarize(text, max_words)\n",
        "                elif method == \"Luhn\":\n",
        "                    summary = luhn_summarize(text, max_words)\n",
        "                else:\n",
        "                    return \"Invalid method selected.\", 0\n",
        "\n",
        "                return summary, word_count(text), word_count(summary)\n",
        "\n",
        "            extractive_generate_button = gr.Button(\"Generate Extractive Summary\")\n",
        "            extractive_generate_button.click(\n",
        "                fn=extractive_summarize,\n",
        "                inputs=[extractive_input_text, extractive_method_choice, extractive_max_words],\n",
        "                outputs=[extractive_output_text, input_word_count, output_word_count]\n",
        "            )\n",
        "\n",
        "        # Abstractive summarization tab\n",
        "        with gr.TabItem(\"Abstractive Summarization\"):\n",
        "            abstractive_method_choice = gr.Dropdown(choices=[\"T5\", \"BART\", \"LLM (Gemini-1.5)\"], label=\"Select Abstractive Method\")\n",
        "            abstractive_max_words = gr.Number(label=\"Max Words\", value=50)\n",
        "\n",
        "            with gr.Row():\n",
        "                # Left column for input text\n",
        "                with gr.Column(scale=1):\n",
        "                    abstractive_input_text = gr.Textbox(label=\"Input Text\", lines=10, placeholder=\"Enter text to summarize\")\n",
        "\n",
        "                # Right column for output text and word counts\n",
        "                with gr.Column(scale=1):\n",
        "                    abstractive_output_text = gr.Textbox(label=\"Abstractive Summary\", placeholder=\"Summary will appear here\")\n",
        "                    input_word_count_abstractive = gr.Textbox(label=\"Input Word Count\", value=\"0\", interactive=False)\n",
        "                    output_word_count_abstractive = gr.Textbox(label=\"Output Word Count\", value=\"0\", interactive=False)\n",
        "\n",
        "            def abstractive_summarize(text, method, max_words):\n",
        "                if method == \"T5\":\n",
        "                    summary = t5_summarize(text, max_words)\n",
        "                elif method == \"BART\":\n",
        "                    summary = bart_summarize(text, max_words)\n",
        "                elif method == \"LLM (Gemini-1.5)\":\n",
        "                    summary = llm_summarize(text, model=\"gemini-1.5-pro\", num_words=max_words)\n",
        "                else:\n",
        "                    return \"Invalid method selected.\", 0\n",
        "\n",
        "                return summary, word_count(text), word_count(summary)\n",
        "\n",
        "            abstractive_generate_button = gr.Button(\"Generate Abstractive Summary\")\n",
        "            abstractive_generate_button.click(\n",
        "                fn=abstractive_summarize,\n",
        "                inputs=[abstractive_input_text, abstractive_method_choice, abstractive_max_words],\n",
        "                outputs=[abstractive_output_text, input_word_count_abstractive, output_word_count_abstractive]\n",
        "            )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "RXBRrJXvp7aW",
        "outputId": "b841e7f0-6b9c-4a08-a386-c53879166572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://60e2d00738b933f2d9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://60e2d00738b933f2d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}