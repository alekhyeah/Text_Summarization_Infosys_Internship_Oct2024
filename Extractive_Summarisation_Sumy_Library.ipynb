{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqzdpu9Yp6_I",
        "outputId": "98df0b6e-6915-40be-ac73-934662d2a21e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=d2447037d24def16fa22e0d5b2541fbcb83f572af3055bbec09bfd49757620de\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7d933438bab0807ff39c77b25d8929c8ce86c34da47ff66f25240ba75762519e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hRAYaNhraRD",
        "outputId": "a57664ee-7ba0-4ea1-83aa-310496a8bfc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''In recent years, the field of artificial intelligence (AI) has made significant strides,\n",
        "impacting various sectors such as healthcare, finance, and transportation.\n",
        "AI technologies, including machine learning and natural language processing,\n",
        "have enabled businesses to analyze vast amounts of data, automate processes,\n",
        "and improve decision-making. For instance, in healthcare,\n",
        "AI algorithms are being used to predict patient outcomes, assist in diagnostics,\n",
        "and personalize treatment plans. Similarly, in finance, AI-driven tools are enhancing fraud detection\n",
        "and enabling high-frequency trading. However, despite these advancements, ethical concerns surrounding AI,\n",
        "such as bias in algorithms and data privacy issues,\n",
        "remain pressing challenges that need to be addressed as the technology continues to evolve.'''\n",
        "\n",
        "# Load necessary packages for text parsing and tokenization\n",
        "from sumy.parsers.plaintext import PlaintextParser  # Used to parse the input text for summarization\n",
        "from sumy.nlp.tokenizers import Tokenizer  # Used to tokenize the text (split into words and sentences)\n",
        "\n",
        "# Creating a text parser using tokenization\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "# Import the TextRankSummarizer from the sumy library\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "# Create an instance of the TextRankSummarizer class\n",
        "summarizer = TextRankSummarizer()\n",
        "\n",
        "# 3 means the summary will consist of 3 sentences\n",
        "summary = summarizer(parser.document, 3)\n",
        "\n",
        "# Initialize an empty string to store the summarized text\n",
        "text_summary = \"\"\n",
        "\n",
        "# Loop through each sentence in the generated summary\n",
        "for sentence in summary:\n",
        "\n",
        "  # Append each sentence to the 'text_summary' string\n",
        "  text_summary += str(sentence)\n",
        "\n",
        "# Print the final summarized text\n",
        "print(text_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRTk3XZXp9mq",
        "outputId": "cd4a5d54-bb49-4d1c-bb07-73b5c503b6bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In recent years, the field of artificial intelligence (AI) has made significant strides, impacting various sectors such as healthcare, finance, and transportation.For instance, in healthcare, AI algorithms are being used to predict patient outcomes, assist in diagnostics, and personalize treatment plans.However, despite these advancements, ethical concerns surrounding AI, such as bias in algorithms and data privacy issues, remain pressing challenges that need to be addressed as the technology continues to evolve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LexRank"
      ],
      "metadata": {
        "id": "CErgzTyLr0k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from the sumy package\n",
        "from sumy.parsers.plaintext import PlaintextParser  # Used to parse plain text documents\n",
        "from sumy.nlp.tokenizers import Tokenizer  # Used to tokenize the text into words and sentences\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer  # Used for summarization using LexRank algorithm\n",
        "\n",
        "# Define the function to summarize text using the LexRank method\n",
        "def sumy_method(text):\n",
        "    # Create a parser for the input text with English tokenizer\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Create an instance of the LexRankSummarizer\n",
        "    summarizer = LexRankSummarizer()\n",
        "\n",
        "    # Summarize the document with 2 sentences\n",
        "    summary = summarizer(parser.document, 2)\n",
        "\n",
        "    # Initialize a list to hold the summarized sentences\n",
        "    dp = []\n",
        "\n",
        "    # Loop through each sentence in the summary\n",
        "    for i in summary:\n",
        "        lp = str(i)  # Convert the sentence to a string\n",
        "        dp.append(lp)  # Append the string to the list\n",
        "\n",
        "    # Join all sentences in the list into a single string\n",
        "    final_sentence = ' '.join(dp)\n",
        "\n",
        "    # Return the final summarized text\n",
        "    return final_sentence"
      ],
      "metadata": {
        "id": "j0u8WPaSr2S1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"In the five matches at the venue in the Indian Premier League held earlier this year, teams crossed the 200-run mark eight times. One can expect the second match between India and Bangladesh can be a high-scoring one as well. In the first match in Gwalior, in imperious India thumped Bangladesh by seven wickets. Opting to ball first, India bundled out Bangladesh for a paltry 127 runs and then romped home in 12 overs with seven wickets in the hut. India is unlikely to make any changes in their playing XIs. For Bangladesh, all the eyes will be on veteran all-rounder Mahmudullah, who has announced his retirement from the T20Is. The third match in Hyderabad will be his last match in this format. Mahmudullah had already announced his retirement from Tests in 2021 but has made it clear that he will continue to feature in ODIs and is eyeing the 2025 Champions Trophy and 2027 ODI World Cup.\"\n",
        "\n",
        "# Call the sumy_method function with the text\n",
        "summary = sumy_method(text)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRQXwbQ_r76B",
        "outputId": "68d31362-1c52-4bc1-bb04-9c38c2f01252"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the five matches at the venue in the Indian Premier League held earlier this year, teams crossed the 200-run mark eight times. One can expect the second match between India and Bangladesh can be a high-scoring one as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luhn"
      ],
      "metadata": {
        "id": "f03G_0F3sEo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from the sumy package\n",
        "from sumy.parsers.plaintext import PlaintextParser  # Used to parse plain text documents\n",
        "from sumy.nlp.tokenizers import Tokenizer  # Used to tokenize the text into words and sentences\n",
        "from sumy.summarizers.luhn import LuhnSummarizer  # Used for summarization using the Luhn algorithm\n",
        "\n",
        "# Define the function to summarize text using the Luhn method\n",
        "def luhn_method(text):\n",
        "    # Create a parser for the input text with English tokenizer\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Create an instance of the LuhnSummarizer\n",
        "    summarizer_luhn = LuhnSummarizer()\n",
        "\n",
        "    # Summarize the document with 2 sentences\n",
        "    summary_1 = summarizer_luhn(parser.document, 2)\n",
        "\n",
        "    # Initialize a list to hold the summarized sentences\n",
        "    dp = []\n",
        "\n",
        "    # Loop through each sentence in the summary\n",
        "    for i in summary_1:\n",
        "        lp = str(i)  # Convert the sentence to a string\n",
        "        dp.append(lp)  # Append the string to the list\n",
        "\n",
        "    # Join all sentences in the list into a single string\n",
        "    final_sentence = ' '.join(dp)\n",
        "\n",
        "    # Return the final summarized text\n",
        "    return final_sentence"
      ],
      "metadata": {
        "id": "YQ_Qn2R8sGH0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"In recent years, the field of artificial intelligence (AI) has made significant strides, impacting various sectors such as healthcare, finance, and transportation.\n",
        "AI technologies, including machine learning and natural language processing, have enabled businesses to analyze vast amounts of data, automate processes, and improve decision-making.\n",
        "For instance, in healthcare, AI algorithms are being used to predict patient outcomes, assist in diagnostics, and personalize treatment plans.\n",
        "Similarly, in finance, AI-driven tools are enhancing fraud detection and enabling high-frequency trading.\n",
        "However, despite these advancements, ethical concerns surrounding AI, such as bias in algorithms and data privacy issues, remain pressing challenges that need to be addressed as the technology continues to evolve.\"\"\"\n",
        "\n",
        "# Call the luhn_method function with the text\n",
        "summary = luhn_method(text)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jt7eCLKsKn7",
        "outputId": "4dc886a2-4172-4808-f910-6c9800ef8a3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In recent years, the field of artificial intelligence (AI) has made significant strides, impacting various sectors such as healthcare, finance, and transportation. However, despite these advancements, ethical concerns surrounding AI, such as bias in algorithms and data privacy issues, remain pressing challenges that need to be addressed as the technology continues to evolve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSA"
      ],
      "metadata": {
        "id": "lXVfJNzssUmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from the sumy package\n",
        "from sumy.parsers.plaintext import PlaintextParser  # Used to parse plain text documents\n",
        "from sumy.nlp.tokenizers import Tokenizer  # Used to tokenize the text into words and sentences\n",
        "from sumy.summarizers.lsa import LsaSummarizer  # Used for summarization using the LSA algorithm\n",
        "\n",
        "# Define the function to summarize text using the LSA method\n",
        "def lsa_method(text):\n",
        "    # Create a parser for the input text with English tokenizer\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Create an instance of the LsaSummarizer\n",
        "    summarizer_lsa = LsaSummarizer()\n",
        "\n",
        "    # Summarize the document with 2 sentences\n",
        "    summary_2 = summarizer_lsa(parser.document, 2)\n",
        "\n",
        "    # Initialize a list to hold the summarized sentences\n",
        "    dp = []\n",
        "\n",
        "    # Loop through each sentence in the summary\n",
        "    for i in summary_2:\n",
        "        lp = str(i)  # Convert the sentence to a string\n",
        "        dp.append(lp)  # Append the string to the list\n",
        "\n",
        "    # Join all sentences in the list into a single string\n",
        "    final_sentence = ' '.join(dp)\n",
        "\n",
        "    # Return the final summarized text\n",
        "    return final_sentence"
      ],
      "metadata": {
        "id": "u5Lf-_sxsVXm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"In recent years, the field of artificial intelligence (AI) has made significant strides, impacting various sectors such as healthcare, finance, and transportation. AI technologies, including machine learning and natural language processing, have enabled businesses to analyze vast amounts of data, automate processes, and improve decision-making. For instance, in healthcare, AI algorithms are being used to predict patient outcomes, assist in diagnostics, and personalize treatment plans. Similarly, in finance, AI-driven tools are enhancing fraud detection and enabling high-frequency trading. However, despite these advancements, ethical concerns surrounding AI, such as bias in algorithms and data privacy issues, remain pressing challenges that need to be addressed as the technology continues to evolve.\"\"\"\n",
        "\n",
        "# Call the lsa_method function with the text\n",
        "summary = lsa_method(text)\n",
        "\n",
        "# Print the generated summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHPEenEFsZqE",
        "outputId": "87043726-4455-49aa-9ae3-a257f3e2ff06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In recent years, the field of artificial intelligence (AI) has made significant strides, impacting various sectors such as healthcare, finance, and transportation. Similarly, in finance, AI-driven tools are enhancing fraud detection and enabling high-frequency trading.\n"
          ]
        }
      ]
    }
  ]
}