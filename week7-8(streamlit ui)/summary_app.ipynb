{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38675,"status":"ok","timestamp":1732705104943,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"8U5KgQhXC9Er","outputId":"65789619-f317-40f2-9280-383c62374572"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install streamlit pyngrok transformers sumy -q\n","!pip install langchain_google_genai langchain_core -q\n","!pip install PyMuPDF -q\n","!pip install python-docx -q\n","!pip install PyPDF2 -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4705,"status":"ok","timestamp":1732705109641,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"vFpAF275LBgF","outputId":"2ca4b71a-46d1-4390-c4e8-1968470c6506"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fpdf2\n","  Downloading fpdf2-2.8.1-py2.py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from fpdf2) (0.7.1)\n","Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (11.0.0)\n","Requirement already satisfied: fonttools>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (4.55.0)\n","Downloading fpdf2-2.8.1-py2.py3-none-any.whl (227 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.8/227.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fpdf2\n","Successfully installed fpdf2-2.8.1\n"]}],"source":["!pip install fpdf2"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20274,"status":"ok","timestamp":1732705129911,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"lU8aqHbCcuZu","outputId":"a1692093-a35e-441c-f3a5-f798915f8d4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -U  langchain langchain-community pypdf langchain-google-genai langgraph --quiet"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1732707964224,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"lBNP_cQY0QL2"},"outputs":[],"source":["app_code = '''\n","import streamlit as st\n","from transformers import BartForConditionalGeneration, BartTokenizer, T5ForConditionalGeneration, T5Tokenizer\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.text_rank import TextRankSummarizer\n","from sumy.summarizers.lsa import LsaSummarizer\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from collections import defaultdict\n","from langchain.document_loaders import PyPDFLoader\n","from fpdf import FPDF\n","from PyPDF2 import PdfReader\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain.chains.summarize import load_summarize_chain\n","from langchain.prompts import PromptTemplate\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.docstore.document import Document\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt_tab')\n","\n","\n","st.set_page_config(layout=\"wide\")\n","\n","\n","def load_and_split_pdf(file_path):\n","    loader = PyPDFLoader(file_path)\n","    documents = loader.load()\n","\n","    # Split the documents into smaller chunks\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n","    docs = text_splitter.split_documents(documents)\n","\n","    return docs\n","\n","\n","def count_words_in_pdf(pdf_path):\n","    try:\n","        with open(pdf_path, 'rb') as pdf_file:\n","            reader =PdfReader(pdf_file)\n","            text = \"\"\n","\n","            for page in reader.pages:\n","                text += page.extract_text()\n","            words = text.split()\n","            return len(words)\n","    except Exception as e:\n","        print(f\"Error reading PDF: {e}\")\n","        return 0\n","\n","def pdf_to_text(pdf_path):\n","    text = \"\"\n","    try:\n","        with open(pdf_path, 'rb') as pdf_file:\n","            reader = PdfReader(pdf_file)\n","            for page in reader.pages:\n","                text += page.extract_text()\n","\n","    except Exception as e:\n","        text = f\"An error occurred: {e}\"\n","    return text\n","\n","\n","def text_to_pdf(text, pdf_path):\n","    # Create an instance of FPDF\n","    pdf = FPDF()\n","    pdf.set_auto_page_break(auto=True, margin=15)\n","    pdf.add_page()\n","    pdf.set_font(\"Arial\", size=12)\n","\n","    # Split text into lines and add them to the PDF\n","    for line in text.split(\". \"):\n","        pdf.cell(200, 10, txt=line, ln=True, align='L')\n","\n","    # Save the PDF to the specified path\n","    pdf.output(pdf_path)\n","\n","\n","def load_llm(model=\"gemini-1.5-flash\"):\n","    llm = ChatGoogleGenerativeAI(\n","        model=model,\n","        temperature=0,\n","        max_tokens=1024,\n","        timeout=None,\n","        max_retries=2\n","    )\n","    return llm\n","\n","def get_prompt_template(num_words):\n","    return ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", f\"Write a concise summary of the following in {num_words} words:  \"),\n","            (\"human\", \"{context}\")\n","        ]\n","    )\n","\n","\n","\n","def get_chain(num_words, model_type, llm):\n","    if model_type == \"refine\":\n","        base_prompt = PromptTemplate(\n","            input_variables=[\"text\"],\n","            template=f\"\"\"\n","            Summarize the following text in {num_words} words or  more:\n","            {{text}}\n","            Summary:\"\"\"\n","        )\n","\n","        refine_prompt = PromptTemplate(\n","            input_variables=[\"existing_summary\", \"text\"],\n","            template=f\"\"\"\n","            Your previous summary is:\n","            {{existing_summary}}\n","            Refine it to ensure it remains concise and exactly {num_words} words or more.\n","            Updated Summary:\"\"\"\n","        )\n","        chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True, question_prompt=base_prompt, refine_prompt=refine_prompt)\n","        return chain\n","\n","    elif model_type == \"map_reduce\":\n","        map_prompt = PromptTemplate(\n","            input_variables=[\"text\"],\n","            template=f\"\"\"\n","            Summarize the following text in {num_words} words :\n","            {{text}}\n","            Summary:\"\"\"\n","        )\n","\n","        combine_prompt = PromptTemplate(\n","            input_variables=[\"context\", \"text\"],\n","            template=f\"\"\"\n","            The following is the current summary:\n","            {{context}}\n","\n","            Refine the summary based on the additional text below. Ensure the final summary is concise\n","            and exactly {num_words} words:\n","            {{text}}\n","            Updated Summary:\"\"\"\n","        )\n","\n","        # Create the Map-Reduce chain\n","        chain = load_summarize_chain(\n","            llm,\n","            chain_type=\"map_reduce\",\n","            verbose=True,\n","            map_prompt=map_prompt,\n","            combine_prompt=combine_prompt\n","        )\n","        return chain\n","\n","    else:\n","        raise ValueError(f\"Unsupported model_type: {model_type}\")\n","\n","\n","\n","def summarize_text_llm(model_type, num_words, inputs):\n","    llm = load_llm(\"gemini-1.5-flash\")\n","\n","    if model_type == \"stuffed_document\":\n","        prompt = get_prompt_template(num_words)\n","        chain = prompt | llm\n","        result = chain.invoke({\n","            \"context\": inputs,\n","            \"num_words\": num_words\n","        })\n","        return result.content\n","\n","    elif model_type ==\"refine\":\n","        loader = PyPDFLoader(inputs)\n","        docs = loader.load_and_split()\n","        chain=get_chain(num_words,model_type,llm)\n","        summary = chain.invoke(docs)\n","        return summary['output_text']\n","\n","    elif model_type ==\"map_reduce\":\n","        # Load and split documents\n","        docs = load_and_split_pdf(inputs)\n","\n","        # Get the chain\n","        chain = get_chain(num_words, model_type, llm)\n","\n","        # Invoke the chain with the documents\n","        summary = chain.invoke({\n","    \"input_documents\": docs,\n","    \"context\": \"\"  # Initializing context with an empty string\n","})\n","        return summary['output_text']\n","\n","def load_abstractive_models():\n","    bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","    bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","    t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n","    t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","    return (bart_model, bart_tokenizer), (t5_model, t5_tokenizer)\n","\n","\n","def abstractive_summary(text, model_type, num_words):\n","    if model_type == \"BART\":\n","        model, tokenizer = load_abstractive_models()[0]\n","    elif model_type == \"T5\":\n","        model, tokenizer = load_abstractive_models()[1]\n","\n","    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","    summary_ids = model.generate(\n","        inputs['input_ids'], max_length=int(num_words * 1.1), min_length=int(num_words *1),\n","        length_penalty=0.05, num_beams=4, early_stopping=False\n","    )\n","    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","\n","def extractive_summary(text, method, num_of_line=2):\n","    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n","    if method == \"TextRank\":\n","        summarizer = TextRankSummarizer()\n","        summary = summarizer(parser.document, num_of_line)\n","        return ' '.join(str(sentence) for sentence in summary)\n","    elif method == \"LSA\":\n","        summarizer = LsaSummarizer()\n","        summary = summarizer(parser.document, num_of_line)\n","        return ' '.join(str(sentence) for sentence in summary)\n","    elif method == \"Frequency\":\n","        return frequency_summary(text, num_of_line)\n","\n","\n","def frequency_summary(text, num_of_line):\n","    stop_words = set(stopwords.words(\"english\"))\n","    words = word_tokenize(text.lower())\n","    freq_table = defaultdict(int)\n","    for word in words:\n","        if word.isalpha() and word not in stop_words:\n","            freq_table[word] += 1\n","\n","    sentences = sent_tokenize(text)\n","    sentence_scores = defaultdict(int)\n","    for sentence in sentences:\n","        for word in word_tokenize(sentence.lower()):\n","            if word in freq_table:\n","                sentence_scores[sentence] += freq_table[word]\n","\n","    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n","    summary = summary_sentences[:num_of_line]\n","    return ' '.join(summary)\n","\n","\n","\n","\n","\n","def main():\n","    st.title(\"Text Summarization App\")\n","    st.write(\"Choose a summarization method:\")\n","\n","    incol, outcol = st.columns(2)\n","\n","    with incol:\n","        intype,method,algorithm = st.columns(3)\n","        with intype:\n","            input_type = st.radio(\"Select Input Type\", [\"Text\", \"Document\"])\n","        with method:\n","            summarization_type = st.radio(\"Select Summarization Method\", [\"Extractive\", \"Abstractive\", \"LLM\"])\n","        with algorithm:\n","            if summarization_type == \"Extractive\":\n","                method = st.radio(\"Select Algorithm\", [\"TextRank\", \"LSA\", \"Frequency\"])\n","            elif summarization_type == \"Abstractive\":\n","                model_type = st.radio(\"Select Algorithm\", [\"BART\", \"T5\"])\n","            elif summarization_type == \"LLM\":\n","                model_type = st.radio(\"Select Algorithm\", [\"stuffed_document\", \"refine\", \"map_reduce\"])\n","        if summarization_type == \"Extractive\":\n","            num_of_lines = st.number_input(\"Number of lines in summary:\", min_value=1, value=2)\n","        else:\n","            num_words = st.number_input(\"Number of words in summary:\", min_value=10, value=100)\n","\n","        if input_type == \"Text\":\n","            text_input = st.text_area(\"Enter text to summarize:\", height=200)\n","        elif input_type == \"Document\":\n","            pdf_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n","            if pdf_file:\n","                # Save the uploaded file to a temporary path\n","                with open(\"temp_uploaded_file.pdf\", \"wb\") as f:\n","                    f.write(pdf_file.read())\n","                pdf_file_path = \"temp_uploaded_file.pdf\"\n","            else:\n","                st.warning(\"Please upload a PDF file.\")\n","                pdf_file_path = None\n","\n","    if st.button(\"Summarize\"):\n","        if input_type ==\"Text\":\n","            in_word= len(text_input.split(\" \"))\n","        elif input_type == \"Document\":\n","            in_word=count_words_in_pdf(pdf_file_path)\n","\n","        if summarization_type == \"Abstractive\":\n","            if input_type==\"Document\":\n","              text_input=pdf_to_text(pdf_file_path)\n","            summary = abstractive_summary(text_input, model_type, num_words)\n","        elif summarization_type == \"Extractive\":\n","            if input_type==\"Document\":\n","              text_input=pdf_to_text(pdf_file_path)\n","            summary = extractive_summary(text_input, method, num_of_lines)\n","        elif summarization_type == \"LLM\":\n","            if model_type == \"stuffed_document\" and input_type==\"Document\":\n","                text_input=pdf_to_text(pdf_file_path)\n","            elif model_type in [\"refine\", \"map_reduce\"] :\n","                if input_type==\"Text\":\n","                    with open(\"temp_uploaded_file.pdf\", \"wb\") as f:\n","                        text_to_pdf(text_input, \"temp_uploaded_file.pdf\")\n","                    pdf_file_path = \"temp_uploaded_file.pdf\"\n","                text_input=pdf_file_path\n","            summary = summarize_text_llm(model_type, num_words, text_input)\n","        with outcol:\n","            st.subheader(\"Summary\")\n","            st.write(summary)\n","            out_word=len(summary.split(\" \"))\n","            out_percent=round((out_word*100/in_word),2)\n","            st.write(f'The text was reduced from {in_word} words to {out_word} words.({out_percent}%)')\n","\n","\n","if __name__ == \"__main__\":\n","    main()'''\n","\n","with open('text_summarization.py', 'w') as f:\n","    f.write(app_code)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2409,"status":"ok","timestamp":1732705132316,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"3JCnueHVqzov"},"outputs":[],"source":["import os\n","from google.colab import userdata\n","os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n","os.environ['ngrok_token']=userdata.get('my_auth_token')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1732705133066,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"pOwe-xg1EsrL","outputId":"f64e8757-5269-46bf-be61-8154cc900610"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!ngrok authtoken $ngrok_token"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1732708921083,"user":{"displayName":"Mohamed Siddiqh","userId":"09755181861592032924"},"user_tz":-330},"id":"Limqb_rHDkl0","outputId":"360124ad-aef1-4be9-ff01-29c02e9e9ce3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit App is live at: NgrokTunnel: \"https://bc35-35-199-53-189.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}],"source":["from pyngrok import ngrok\n","\n","public_url = ngrok.connect(8501)\n","print(f\"Streamlit App is live at: {public_url}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jQwC1McHOHT","outputId":"eb6222f3-abc8-4c33-9d52-5fbd43b9a68f"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.199.53.189:8501\u001b[0m\n","\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:pyngrok.process.ngrok:t=2024-11-27T12:02:06+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"]}],"source":["!streamlit run text_summarization.py"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1OH_K4kTpJ_TrF0YNLKHK70BEbLTExUQB","authorship_tag":"ABX9TyMEYKpqIxRyjInDA2ybnbw8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}