{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd6c0567",
      "metadata": {
        "id": "fd6c0567"
      },
      "outputs": [],
      "source": [
        "data = \"My name is Bhaskar Rai. It is my pleasure to take part in Infosys Springboard Internship. And I am working on Text Summarization Project.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c39e8e9d",
      "metadata": {
        "id": "c39e8e9d"
      },
      "outputs": [],
      "source": [
        "# functions from NLTK (Natural Language Toolkit)\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # Used for splitting the text into words and sentences\n",
        "from nltk.corpus import stopwords  # Stopwords are common words (e.g., 'is', 'the') that don't add much meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab415592",
      "metadata": {
        "id": "ab415592"
      },
      "outputs": [],
      "source": [
        "# Define a function that will summarize the text\n",
        "def solve(text):\n",
        "\n",
        "  # Load English stopwords (common words that are not important for summarization)\n",
        "  stopwords1 = set(stopwords.words(\"english\"))\n",
        "\n",
        "  # Tokenize the text into words\n",
        "  words = word_tokenize(text)\n",
        "\n",
        "  # Create a dictionary to store the frequency of each word\n",
        "  freqTable = {}\n",
        "\n",
        "  # Iterate through each word in the text\n",
        "  for word in words:\n",
        "\n",
        "    # Convert the word to lowercase to avoid case sensitivity\n",
        "    word = word.lower()\n",
        "\n",
        "    # Skip the word if it is a stopword (common word like 'is', 'the')\n",
        "    if word in stopwords1:\n",
        "      continue\n",
        "\n",
        "    # If the word is already in the frequency table, increase its count\n",
        "    if word in freqTable:\n",
        "      freqTable[word] += 1\n",
        "    else:\n",
        "      # If the word is not in the table, add it and set its count to 1\n",
        "      freqTable[word] = 1\n",
        "\n",
        "  # Tokenize the text into sentences\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  # Create a dictionary to store the value (importance score) of each sentence\n",
        "  sentenceValue = {}\n",
        "\n",
        "  # For each sentence in the text\n",
        "  for sentence in sentences:\n",
        "\n",
        "    # For each word and its frequency from the freqTable\n",
        "    for word, freq in freqTable.items():\n",
        "\n",
        "      # If the word is found in the sentence (convert to lowercase for case-insensitive match)\n",
        "      if word in sentence.lower():\n",
        "\n",
        "        # Add the frequency of the word to the sentence's value\n",
        "        if sentence in sentenceValue:\n",
        "          sentenceValue[sentence] += freq\n",
        "        else:\n",
        "          # If it's the first time encountering this sentence, initialize its value with the word's frequency\n",
        "          sentenceValue[sentence] = freq\n",
        "\n",
        "  # Sum up all the sentence values to calculate the total score\n",
        "  sumValues = 0\n",
        "  for sentence in sentenceValue:\n",
        "    sumValues += sentenceValue[sentence]\n",
        "\n",
        "  # Calculate the average sentence value to decide which sentences are important\n",
        "  average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "  # Create a summary by selecting sentences with a score higher than 1.2 times the average value\n",
        "  summary = ''\n",
        "  for sentence in sentences:\n",
        "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "      # Add high-scoring sentences to the summary\n",
        "      summary += \" \" + sentence\n",
        "\n",
        "  # Return the final summary\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a424b38e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a424b38e",
        "outputId": "e7330e3c-8d0f-4905-f905-788436b83051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21e659b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21e659b1",
        "outputId": "2dffc5e3-681b-4385-c5db-1ab44416bb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It is my pleasure to take part in Infosys Springboard Internship.\n"
          ]
        }
      ],
      "source": [
        "# Call the solve function with the text data\n",
        "summary = solve(data)\n",
        "\n",
        "# Print the summary generated by the function\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
