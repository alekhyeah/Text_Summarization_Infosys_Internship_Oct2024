{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language."
      ],
      "metadata": {
        "id": "1coI53S3JxAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of Text Summarization Techniques:\n",
        "Abstractive Summarization: Generates a summary that captures the main ideas by rephrasing and paraphrasing the original content, often using deep learning models. Example techniques include:\n",
        "\n",
        "Sequence-to-sequence models with attention mechanisms.\n",
        "Transformer models like BERT and GPT.\n",
        "Extractive Summarization: Selects and extracts key sentences or phrases from the original text to create a summary, preserving the original wording.\n"
      ],
      "metadata": {
        "id": "GuyDkF2OJw8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Extractive Summarization Methods\n",
        "\n",
        "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
        "   - Weighs the importance of words in a document relative to a corpus, allowing the selection of sentences that contain the most important terms.\n",
        "\n",
        "2. **TextRank**:\n",
        "   - A graph-based ranking algorithm that identifies important sentences based on their connections within the text, similar to the PageRank algorithm used by Google.\n",
        "\n",
        "3. **LexRank**:\n",
        "   - Another graph-based method that identifies sentence importance by measuring the centrality of sentences in a graph, where sentences are nodes and edges represent similarities.\n",
        "\n",
        "4. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
        "   - Utilizes the pre-trained BERT model to generate embeddings for sentences, which can then be clustered or ranked based on similarity to create summaries.\n",
        "\n",
        "5. **LSA (Latent Semantic Analysis)**:\n",
        "   - A technique that reduces the dimensionality of the text data by identifying patterns in word usage, allowing the extraction of semantically relevant sentences.\n",
        "\n",
        "6. **LDA (Latent Dirichlet Allocation)**:\n",
        "   - A topic modeling technique that can also be used for extractive summarization by identifying the most representative sentences for each topic in a document.\n",
        "\n",
        "7. **SumBasic**:\n",
        "   - A probabilistic extractive summarization method that selects sentences based on the frequency of words in the summary and document, ensuring that more important words are included.\n",
        "\n",
        "8. **Centroid-based Summarization**:\n",
        "   - Identifies the centroid of a set of sentences (i.e., the average representation) and selects sentences closest to this centroid as the summary.\n",
        "\n",
        "9. **MMR (Maximal Marginal Relevance)**:\n",
        "   - A method that combines relevance and diversity by selecting sentences that maximize relevance while minimizing redundancy with the already selected sentences.\n",
        "\n",
        "10. **Graph-Based Methods**:\n",
        "   - Techniques like **HITS (Hyperlink-Induced Topic Search)** can also be adapted for extractive summarization by assessing the importance of sentences based on their connectivity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qOv85FUMJw69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QYXvtbzDh0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cea80e-7ff5-44e9-d20c-21db58596ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=6349d606352fc497b626c74be89cf242a9ca7291f8b620b5b4c3cd105d278f08\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=398ea0410e4190be27fdb3caff6620b42079b69d01e5c49d13644e555f140719\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.0\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart_open>=1.8.1->gensim==3.8.3) (1.16.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install numpy scikit-learn\n",
        "!pip install gensim\n",
        "!pip install sumy\n",
        "!pip install sentence-transformers\n",
        "!pip install gensim==3.8.3"
      ]
    },
    {
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from transformers import pipeline\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "utRgp6n-Er58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23a0df4-751f-4381-d0dc-06edd1579da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    from gensim.summarization.summarizer import summarize as gensim_summarize\n",
        "except ImportError:\n",
        "    print(\"gensim.summarization not found. Please make sure Gensim 3.8.3 or lower is correctly installed and the kernel is restarted.\")\n"
      ],
      "metadata": {
        "id": "IC_vxLXjKoxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d43263-c120-4c0b-bc68-29a0827b71be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gensim.summarization not found. Please make sure Gensim 3.8.3 or lower is correctly installed and the kernel is restarted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"\n",
        "Artificial intelligence (AI) is transforming industries by automating tasks, enhancing decision-making processes, and improving efficiency.\n",
        "AI technologies, such as machine learning and natural language processing, are being integrated into various sectors like healthcare, finance, and manufacturing.\n",
        "In healthcare, AI is helping doctors to diagnose diseases faster and more accurately, while in finance, AI algorithms are optimizing trading and risk management.\n",
        "The ability of AI to analyze large amounts of data and provide actionable insights is making it indispensable in today’s data-driven world.\n",
        "\n",
        "Despite the benefits, there are also concerns about AI’s impact on jobs and society.\n",
        "Automation could lead to job displacement in certain sectors, particularly those that involve repetitive tasks.\n",
        "Moreover, ethical issues such as data privacy, algorithmic bias, and transparency remain challenges that need to be addressed.\n",
        "As AI continues to evolve, it is crucial to ensure that its development is guided by ethical considerations, and that regulations are put in place to mitigate its potential negative effects on society.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GFya1YiIKfR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#K-Means Clustering Summarization\n",
        "def kmeans_summarization(text, max_sentences=5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    n_clusters = min(max_sentences, len(sentences))\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    closest_sentences = []\n",
        "    for i in range(n_clusters):\n",
        "        centroid = kmeans.cluster_centers_[i]\n",
        "        closest_sentence_idx = np.argmin(np.linalg.norm(tfidf_matrix - centroid, axis=1))\n",
        "        closest_sentences.append(sentences[closest_sentence_idx])\n",
        "\n",
        "    return ' '.join(closest_sentences)\n",
        "\n",
        "#KL-Sum Summarization (using Luhn as substitute)\n",
        "def klsum_summarization(text, max_sentences=5):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    summary = summarizer(parser.document, max_sentences)\n",
        "\n",
        "    return ' '.join(str(sentence) for sentence in summary)\n",
        "\n",
        "# Luhn Summarizer\n",
        "def luhn_summarization(text, max_sentences=5):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    summary = summarizer(parser.document, max_sentences)\n",
        "\n",
        "    return ' '.join(str(sentence) for sentence in summary)\n",
        "\n",
        "#BERT Summarizer\n",
        "def bert_summarization(text, max_sentences=5):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "    sentences = sent_tokenize(text)\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "    # Rank sentences based on cosine similarity with the mean sentence\n",
        "    mean_embedding = embeddings.mean(dim=0)\n",
        "    similarities = util.pytorch_cos_sim(mean_embedding, embeddings).squeeze().cpu().numpy()\n",
        "\n",
        "    top_sentence_indices = similarities.argsort()[-max_sentences:][::-1]\n",
        "    top_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "#LexRank Summarizer\n",
        "def lexrank_summarization(text, max_sentences=5):\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, max_sentences)\n",
        "    return ' '.join(str(sentence) for sentence in summary)\n",
        "\n",
        "#Transformer Summarizer (using Hugging Face)\n",
        "def transformer_summarization(text, max_sentences=5):\n",
        "    from transformers import pipeline\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summary = summarizer(text, max_length=max_sentences*20, min_length=max_sentences*5, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vGdUL55pHrH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def display_summary(text, technique, max_sentences=5):\n",
        "    if technique == \"K-Means\":\n",
        "        summary = kmeans_summarization(text, max_sentences)\n",
        "    elif technique == \"KL-Sum\":\n",
        "        summary = klsum_summarization(text, max_sentences)\n",
        "    elif technique == \"Luhn\":\n",
        "        summary = luhn_summarization(text, max_sentences)\n",
        "    elif technique == \"BERT\":\n",
        "        summary = bert_summarization(text, max_sentences)\n",
        "    elif technique == \"LexRank\":\n",
        "        summary = lexrank_summarization(text, max_sentences)\n",
        "    elif technique == \"Transformer\":\n",
        "        summary = transformer_summarization(text, max_sentences)\n",
        "    else:\n",
        "        print(\"Invalid technique selected.\")\n",
        "        return None\n",
        "\n",
        "    original_words = len(text.split())\n",
        "    summarized_words = len(summary.split())\n",
        "    print(f\"\\n{technique} Summary ({summarized_words}/{original_words} words):\\n{summary}\")\n",
        "    return summary"
      ],
      "metadata": {
        "id": "n9vXJ8hGTTIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def choose_summarization_technique():\n",
        "    print(\"Choose a summarization technique:\")\n",
        "    print(\"1. K-Means Clustering\")\n",
        "    print(\"2. KL-Sum\")\n",
        "    print(\"3. Luhn Summarizer\")\n",
        "    print(\"4. BERT Summarizer\")\n",
        "    print(\"5. LexRank Summarizer\")\n",
        "    print(\"6. Transformer Summarizer\")\n",
        "\n",
        "    choice = input(\"Enter the number of your chosen technique: \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        return \"K-Means\"\n",
        "    elif choice == \"2\":\n",
        "        return \"KL-Sum\"\n",
        "    elif choice == \"3\":\n",
        "        return \"Luhn\"\n",
        "    elif choice == \"4\":\n",
        "        return \"BERT\"\n",
        "    elif choice == \"5\":\n",
        "        return \"LexRank\"\n",
        "    elif choice == \"6\":\n",
        "        return \"Transformer\"\n",
        "    else:\n",
        "        print(\"Invalid choice. Please choose a valid technique.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def summarization_pipeline(text):\n",
        "    # Call choose_summarization_technique to get the user's choice\n",
        "    technique = choose_summarization_technique()\n",
        "    if technique:\n",
        "        display_summary(text, technique)\n",
        "\n",
        "\n",
        "summarization_pipeline(text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj1hIB0xvubl",
        "outputId": "18898fa0-a6cf-4ef6-d405-2c2099e282ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a summarization technique:\n",
            "1. K-Means Clustering\n",
            "2. KL-Sum\n",
            "3. Luhn Summarizer\n",
            "4. BERT Summarizer\n",
            "5. LexRank Summarizer\n",
            "6. Transformer Summarizer\n",
            "Enter the number of your chosen technique: 1\n",
            "\n",
            "K-Means Summary (106/162 words):\n",
            "Automation could lead to job displacement in certain sectors, particularly those that involve repetitive tasks. As AI continues to evolve, it is crucial to ensure that its development is guided by ethical considerations, and that regulations are put in place to mitigate its potential negative effects on society. \n",
            "Artificial intelligence (AI) is transforming industries by automating tasks, enhancing decision-making processes, and improving efficiency. AI technologies, such as machine learning and natural language processing, are being integrated into various sectors like healthcare, finance, and manufacturing. The ability of AI to analyze large amounts of data and provide actionable insights is making it indispensable in today’s data-driven world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vAvaO09sYdWF"
      }
    }
  ]
}