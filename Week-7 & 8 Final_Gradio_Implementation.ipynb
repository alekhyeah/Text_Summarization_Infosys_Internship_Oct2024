{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Gradio Implementation"
      ],
      "metadata": {
        "id": "mFe-wBhTh3VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio and dependencies installation\n",
        "!pip install gradio --quiet\n",
        "!pip install sumy --quiet\n",
        "!pip install langchain_google_genai --quiet\n",
        "!pip install pypdf --quiet\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S4rJDvUnZGv",
        "outputId": "9180b909-afd9-4710-9d13-d0685eedccc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCA1pI04nb9g",
        "outputId": "9ed2ab8c-b0f5-4bd9-b0d9-5c231e1ebdbd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency-based summarization\n",
        "def frequency_based_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        words = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_scores = {}\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in word_freq:\n",
        "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_freq[word]\n",
        "\n",
        "        sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "\n",
        "        for sentence in sorted_sentences:\n",
        "            word_count += len(word_tokenize(sentence))\n",
        "            if word_count <= max_words:\n",
        "                summary.append(sentence)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LexRank summarization\n",
        "def lexrank_summary(text, max_words):\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        summarizer = LexRankSummarizer()\n",
        "        sentences = parser.document.sentences\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for sentence in summarizer(parser.document, len(sentences)):\n",
        "            sentence_words = len(word_tokenize(str(sentence)))\n",
        "            if word_count + sentence_words <= int(max_words):\n",
        "                summary.append(str(sentence))\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# TextRank summarization\n",
        "def textrank_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        clean_sentences = []\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        for sentence in sentences:\n",
        "            words = word_tokenize(sentence.lower())\n",
        "            words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "            clean_sentences.append(\" \".join(words))\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(graph)\n",
        "        ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for _, sentence in ranked_sentences:\n",
        "            sentence_words = len(word_tokenize(sentence))\n",
        "            if word_count + sentence_words <= max_words:\n",
        "                summary.append(sentence)\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# T5 summarization\n",
        "def t5_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "        input_text = \"summarize: \" + text\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# BART summarization\n",
        "def bart_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "        inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LLM (Gemini) summarization\n",
        "def llm_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, max_output_tokens=max_words)\n",
        "        prompt = f\"Summarize the following text in approximately {max_words} words:\\n\\n{text}\"\n",
        "        result = llm(prompt)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# Advanced LangChain Summarization Techniques\n",
        "def map_reduce_summary(text, max_words):\n",
        "    try:\n",
        "        # Initialize the LLM\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, max_output_tokens=max_words)\n",
        "\n",
        "        # Split the text into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "        # Create documents\n",
        "        docs = [Document(page_content=text)]\n",
        "\n",
        "        # Load map-reduce summarization chain\n",
        "        chain = load_summarize_chain(\n",
        "            llm,\n",
        "            chain_type=\"map_reduce\",\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Run the chain\n",
        "        summary = chain.run(docs)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error in Map Reduce Summary: {str(e)}\"\n",
        "\n",
        "def iterative_refinement_summary(text, max_words):\n",
        "    try:\n",
        "        # Initialize the LLM\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, max_output_tokens=max_words)\n",
        "\n",
        "        # Map prompt\n",
        "        map_prompt = PromptTemplate(\n",
        "            template=\"\"\"Write a concise summary of the following text:\n",
        "            \"{text}\"\n",
        "            CONCISE SUMMARY:\"\"\",\n",
        "            input_variables=[\"text\"]\n",
        "        )\n",
        "\n",
        "        # Refine prompt\n",
        "        refine_prompt = PromptTemplate(\n",
        "            template=\"\"\"You are an expert summarizer.\n",
        "            First, review the existing summary and the new piece of text.\n",
        "            Then, refine the summary to include the most important information,\n",
        "            ensuring it captures the key points while staying within the word limit.\n",
        "            Maintain the word limit strictly.\n",
        "            Refined Summary:\"\"\",\n",
        "            input_variables=[\"text\"]\n",
        "        )\n",
        "\n",
        "        # Split the text into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        docs = text_splitter.create_documents([text])\n",
        "\n",
        "        # Create the map chain\n",
        "        map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "        # Create a chain to combine documents\n",
        "        combine_documents_chain = StuffDocumentsChain(\n",
        "            llm_chain=map_chain,\n",
        "            document_variable_name=\"text\"\n",
        "        )\n",
        "\n",
        "        # Create the summary chain\n",
        "        summary_chain = load_summarize_chain(\n",
        "            llm,\n",
        "            chain_type=\"refine\",\n",
        "            question_prompt=map_prompt,\n",
        "            refine_prompt=refine_prompt,\n",
        "            document_variable_name=\"text\",\n",
        "            return_intermediate_steps=False\n",
        "        )\n",
        "\n",
        "        # Run the chain\n",
        "        summary = summary_chain.run(docs)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error in Iterative Refinement Summary: {str(e)}\"\n",
        "\n",
        "def pdf_summarizer(pdf_file, method, max_words):\n",
        "    try:\n",
        "        # Read PDF\n",
        "        reader = PdfReader(pdf_file)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # Choose summarization method\n",
        "        if method == \"Map Reduce\":\n",
        "            return map_reduce_summary(text, max_words)\n",
        "        elif method == \"Iterative Refinement\":\n",
        "            return iterative_refinement_summary(text, max_words)\n",
        "        elif method == \"T5\":\n",
        "            return t5_summary(text, max_words)\n",
        "        elif method == \"BART\":\n",
        "            return bart_summary(text, max_words)\n",
        "        elif method == \"LLM (Gemini)\":\n",
        "            return llm_summary(text, max_words)\n",
        "        else:\n",
        "            return \"Invalid summarization method selected.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in PDF Summarization: {str(e)}\"\n",
        "\n",
        "# Extractive Summarization\n",
        "def extractive_summarize(text, algorithm, max_words):\n",
        "    if algorithm == \"Frequency-based\":\n",
        "        return frequency_based_summary(text, max_words)\n",
        "    elif algorithm == \"LexRank\":\n",
        "        return lexrank_summary(text, max_words)\n",
        "    elif algorithm == \"TextRank\":\n",
        "        return textrank_summary(text, max_words)\n",
        "    else:\n",
        "        return \"Invalid extractive summarization algorithm.\"\n",
        "\n",
        "# Abstractive Summarization\n",
        "def abstractive_summarize(text, algorithm, max_words):\n",
        "    if algorithm == \"T5\":\n",
        "        return t5_summary(text, max_words)\n",
        "    elif algorithm == \"BART\":\n",
        "        return bart_summary(text, max_words)\n",
        "    else:\n",
        "        return \"Invalid abstractive summarization algorithm.\"\n",
        "\n",
        "# LLM-based Summarization\n",
        "def llm_summarize(text, algorithm, max_words):\n",
        "    if algorithm == \"LLM (Gemini)\":\n",
        "        return llm_summary(text, max_words)\n",
        "    elif algorithm == \"Iterative Refinement\":\n",
        "        return iterative_refinement_summary(text, max_words)\n",
        "    elif algorithm == \"Map Reduce\":\n",
        "        return iterative_refinement_summary(text, max_words)\n",
        "    else:\n",
        "        return \"Invalid LLM summarization algorithm.\"\n",
        "\n",
        "# PDF Summarization\n",
        "def pdf_summarizer(pdf_file, summarization_type, algorithm, max_words):\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_file)\n",
        "        text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "\n",
        "        if summarization_type == \"Extractive\":\n",
        "            return extractive_summarize(text, algorithm, max_words)\n",
        "        elif summarization_type == \"Abstractive\":\n",
        "            return abstractive_summarize(text, algorithm, max_words)\n",
        "        elif summarization_type == \"LLM\":\n",
        "            return llm_summarize(text, algorithm, max_words)\n",
        "        else:\n",
        "            return \"Invalid summarization type.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in PDF summarization: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Text Summarization App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Summarization\"):\n",
        "            gr.Markdown(\"## Select Summarization Type and Method\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    input_type = gr.Radio(\n",
        "                        [\"Text Input\", \"PDF Upload\"],\n",
        "                        label=\"Input Type\",\n",
        "                        value=\"Text Input\"\n",
        "                    )\n",
        "\n",
        "                    # Text Input\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"Input Text\",\n",
        "                        lines=5,\n",
        "                        placeholder=\"Enter text here\",\n",
        "                        visible=True\n",
        "                    )\n",
        "\n",
        "                    # PDF Upload\n",
        "                    pdf_input = gr.File(\n",
        "                        label=\"Upload PDF\",\n",
        "                        type=\"filepath\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        visible=False\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    summarization_type = gr.Radio(\n",
        "                        [\"Extractive\", \"Abstractive\", \"LLM\"],\n",
        "                        label=\"Summarization Type\",\n",
        "                        value=\"Extractive\"\n",
        "                    )\n",
        "\n",
        "                    method_dropdown = gr.Dropdown(\n",
        "                        label=\"Algorithm\",\n",
        "                        visible=True\n",
        "                    )\n",
        "\n",
        "                    words = gr.Textbox(\n",
        "                        label=\"Maximum Words\",\n",
        "                        value=\"100\"\n",
        "                    )\n",
        "\n",
        "            # Output\n",
        "            output = gr.Textbox(\n",
        "                label=\"Summary\",\n",
        "                lines=5\n",
        "            )\n",
        "\n",
        "            # Generate Button\n",
        "            button = gr.Button(\"Generate Summary\")\n",
        "\n",
        "            # Input Type Toggle\n",
        "            def toggle_inputs(choice):\n",
        "                if choice == \"Text Input\":\n",
        "                    return {\n",
        "                        text_input: gr.update(visible=True),\n",
        "                        pdf_input: gr.update(visible=False)\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        text_input: gr.update(visible=False),\n",
        "                        pdf_input: gr.update(visible=True)\n",
        "                    }\n",
        "\n",
        "            input_type.change(\n",
        "                toggle_inputs,\n",
        "                inputs=input_type,\n",
        "                outputs=[text_input, pdf_input]\n",
        "            )\n",
        "\n",
        "            # Update Algorithms Dropdown\n",
        "            def update_algorithms(summarization_type):\n",
        "                if summarization_type == \"Extractive\":\n",
        "                    return gr.update(choices=[\"Frequency-based\", \"LexRank\", \"TextRank\"], visible=True)\n",
        "                elif summarization_type == \"Abstractive\":\n",
        "                    return gr.update(choices=[\"T5\", \"BART\"], visible=True)\n",
        "                elif summarization_type == \"LLM\":\n",
        "                    return gr.update(choices=[\"LLM (Gemini)\", \"Map Reduce\", \"Iterative Refinement\"], visible=True)\n",
        "                else:\n",
        "                    return gr.update(visible=False)\n",
        "\n",
        "            summarization_type.change(\n",
        "                update_algorithms,\n",
        "                inputs=summarization_type,\n",
        "                outputs=method_dropdown\n",
        "            )\n",
        "\n",
        "            # Summarization Logic\n",
        "            def summarize(input_type, text, pdf, summarization_type, method, max_words):\n",
        "                if input_type == \"Text Input\":\n",
        "                    if summarization_type == \"Extractive\":\n",
        "                        return extractive_summarize(text, method, max_words)\n",
        "                    elif summarization_type == \"Abstractive\":\n",
        "                        return abstractive_summarize(text, method, max_words)\n",
        "                    elif summarization_type == \"LLM\":\n",
        "                        return llm_summarize(text, method, max_words)\n",
        "                    else:\n",
        "                        return \"Invalid summarization type.\"\n",
        "                else:  # PDF Upload\n",
        "                    return pdf_summarizer(pdf, summarization_type, method, max_words)\n",
        "\n",
        "            button.click(\n",
        "                summarize,\n",
        "                inputs=[input_type, text_input, pdf_input, summarization_type, method_dropdown, words],\n",
        "                outputs=output\n",
        "            )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "yUXKpIZiaK0x",
        "outputId": "44650117-54bc-41d9-b322-b5dea29dcc6d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d36279927f4896acbd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d36279927f4896acbd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MrKc3Q5iMHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
