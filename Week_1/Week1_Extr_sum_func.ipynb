{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7K_nnBlEUMT",
        "outputId": "552b0811-e134-4d5b-f10b-c9b0f8ab689b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ipo_GVlK3PDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4140f26-b474-404b-a738-0ead11ad8899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose summarization method (frequency, lsa, luhn, lex_rank): lex_rank\n",
            "Summary:\n",
            "Artificial Intelligence (AI) is revolutionizing multiple industries and transforming the way we live and work. AI technologies, including machine learning, natural language processing, and robotics, are being integrated into applications like healthcare, finance, and transportation.\n",
            "\n",
            "Number of words in the original text: 122\n",
            "Number of words in the summary: 35\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Frequency-based Summarization\n",
        "def frequency_based_summarization(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "    max_freq = max(word_freq.values())\n",
        "    word_freq = {word: freq / max_freq for word, freq in word_freq.items()}\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_freq:\n",
        "                sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_freq[word]\n",
        "\n",
        "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "    summary_sentences = sorted_sentences[:3]\n",
        "\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# LSA Summarization\n",
        "def lsa_summarization(text, sentences_count=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# Luhn Summarization\n",
        "def luhn_summarization(text, sentences_count=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# LexRank Summarization\n",
        "def lex_rank_summarization(text, sentences_count=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# Main function to call summarization methods\n",
        "def summarize_text(text, method=\"frequency\", sentences_count=2):\n",
        "    if method == \"frequency\":\n",
        "        return frequency_based_summarization(text)\n",
        "    elif method == \"lsa\":\n",
        "        return lsa_summarization(text, sentences_count)\n",
        "    elif method == \"luhn\":\n",
        "        return luhn_summarization(text, sentences_count)\n",
        "    elif method == \"lex_rank\":\n",
        "        return lex_rank_summarization(text, sentences_count)\n",
        "    else:\n",
        "        return \"Invalid method selected.\"\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is revolutionizing multiple industries and transforming the way we live and work.\n",
        "AI technologies, including machine learning, natural language processing, and robotics, are being integrated into applications like healthcare, finance, and transportation.\n",
        "In healthcare, AI is improving diagnostics and treatment, making processes faster and more accurate. In finance, it helps detect fraudulent transactions and manage risks effectively.\n",
        "AI-powered self-driving cars are reshaping transportation by reducing accidents and optimizing traffic flow. Despite its benefits, AI poses ethical challenges, such as privacy concerns and potential job displacement.\n",
        "Governments and organizations are working on policies to address these issues while fostering innovation in AI. The future of AI looks promising, but it requires careful consideration of both its potential and its risks.\n",
        "\"\"\"\n",
        "\n",
        "# User input for method\n",
        "method_choice = input(\"Choose summarization method (frequency, lsa, luhn, lex_rank): \")\n",
        "summary = summarize_text(text, method=method_choice.lower())\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "# Calculate and print the number of words\n",
        "original_word_count = len(text.split())\n",
        "summary_word_count = len(summary.split())\n",
        "\n",
        "print(f\"\\nNumber of words in the original text: {original_word_count}\")\n",
        "print(f\"Number of words in the summary: {summary_word_count}\")\n",
        "\n"
      ]
    }
  ]
}