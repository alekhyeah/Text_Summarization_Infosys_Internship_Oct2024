{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqTnOAR_z0BK",
        "outputId": "85752467-4cc4-48ef-8ca7-b6a24046ba31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, punkt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nabbAg3iJBC3",
        "outputId": "b94b06d1-4822-44a7-961a-4c8fad894ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Frequency-Based Text Summarization model"
      ],
      "metadata": {
        "id": "164eARkDHm73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_based_summary(text, num_sentences=2):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in words:\n",
        "        if word in word_frequencies:\n",
        "            word_frequencies[word] += 1\n",
        "        else:\n",
        "            word_frequencies[word] = 1\n",
        "\n",
        "\n",
        "    max_freq = max(word_frequencies.values())\n",
        "    word_frequencies = {word: freq / max_freq for word, freq in word_frequencies.items()}\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_word_count = len(word_tokenize(sentence))\n",
        "        sentence_word_count_excluding_stopwords = len([word for word in word_tokenize(sentence.lower()) if word in word_frequencies])\n",
        "\n",
        "        score = sum(word_frequencies.get(word, 0) for word in word_tokenize(sentence.lower()))\n",
        "\n",
        "        if sentence_word_count_excluding_stopwords > 0:\n",
        "            sentence_scores[sentence] = score / sentence_word_count_excluding_stopwords\n",
        "\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "text = \"\"\"Extractive summarization is a text summarization technique based on identifying and separating the primary sentences or phrases in the source text to create summary. The extractive summarization systems employ\n",
        "statistical algorithms and linguistic analysis to assess word frequency, sentence position, and keyword occurrence to gauge the importance of each type of textual input.\n",
        "The prioritized sentences are then placed together to develop a brief, information summary. The primary benefit of extractive summarization is its simplicity and the ability for computational deployment.\n",
        "Additionally, the process is relatively straight forward, as the summary is based on the pre-existing text and its extraction.\n",
        "However, in the operational mode, the summaries may lose interpersonal aspects and lack a wholistic context.\"\"\"\n",
        "\n",
        "summary = frequency_based_summary(text, num_sentences=3)\n",
        "print(\"Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GW4biBD0hWA",
        "outputId": "7b90d501-90e0-4849-dcf7-0f715e17cd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " Extractive summarization is a text summarization technique based on identifying and separating the primary sentences or phrases in the source text to create summary. The primary benefit of extractive summarization is its simplicity and the ability for computational deployment. Additionally, the process is relatively straight forward, as the summary is based on the pre-existing text and its extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- LexRank\n",
        " LexRank treats sentences as nodes in a graph and considers how they are interconnected based on content similarity. Sentences that are more central to the text’s meaning (connected to many other important sentences) are selected for the summary."
      ],
      "metadata": {
        "id": "bgftCuYbJsA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_IbpPqJNsUB",
        "outputId": "599ae795-beeb-4953-f1ed-c040cf4c708c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=18cf0ce03956f9f46b5c00e1704d72d58c4b348f80607e396febf04b08b06b34\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=cced708f996833490e4d3fa4f080dd3789e9b9d49a2f1bb89d916014eb3d3864\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sumy_method(text):\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "  summarizer = LexRankSummarizer()\n",
        "#Summarize the document with 2 sentences\n",
        "  summary = summarizer(parser.document, 2)\n",
        "  dp = []\n",
        "  for i in summary:\n",
        "    lp = str(i)\n",
        "    dp.append(lp)\n",
        "    final_sentence = '\\n'.join(dp)\n",
        "  return final_sentence\n",
        "\n",
        "text= \"\"\"On 24 February 2022, Russia invaded Ukraine in a major escalation of the Russo-Ukrainian War, which started in 2014.\n",
        "The invasion, the largest conflict in Europe since World War II, has caused hundreds of thousands of military casualties and tens\n",
        "of thousands of Ukrainian civilian casualties. As of 2024, Russian troops occupy about 20% of Ukraine. From a population of 41 million,\n",
        "about 8 million Ukrainians had been internally displaced and more than 8.2 million had fled the country by April 2023, creating Europe's largest\n",
        "refugee crisis since World War II.\"\"\"\n",
        "\n",
        "print(\"Summary:\\n\", sumy_method(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFVrLga13GS7",
        "outputId": "0656f0ca-8042-4ef6-f7c5-ab962d29b28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " On 24 February 2022, Russia invaded Ukraine in a major escalation of the Russo-Ukrainian War, which started in 2014.\n",
            "The invasion, the largest conflict in Europe since World War II, has caused hundreds of thousands of military casualties and tens of thousands of Ukrainian civilian casualties.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- TextRank\n",
        "TextRank is an algorithm inspired by Google's PageRank, which is used for ranking web pages. In the context of text summarization, it works by building a graph where the nodes represent sentences, and edges represent the similarity between them. Sentences that are similar to many other important sentences (according to their content) are ranked higher, and the top-ranked sentences are selected for the summary.\n",
        "\n"
      ],
      "metadata": {
        "id": "fRlOmvzhQNao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK stopwords and punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fwczF7QSHhG",
        "outputId": "7e89974f-2189-481d-c614-9af711ece2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank_summary(text, num_sentences=2):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    def preprocess_sentence(sentence):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        return ' '.join([word for word in words if word.isalnum() and word not in stop_words])\n",
        "\n",
        "    processed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(processed_sentences).toarray()\n",
        "\n",
        "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
        "    summary_sentences = [sentence for _, sentence in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    summary = ''.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "text = \"\"\"On 24 February 2022, Russia invaded Ukraine in a major escalation of the Russo-Ukrainian War, which started in 2014.\n",
        "The invasion, the largest conflict in Europe since World War II, has caused hundreds of thousands of military casualties and tens\n",
        "of thousands of Ukrainian civilian casualties. As of 2024, Russian troops occupy about 20% of Ukraine. From a population of 41 million,\n",
        "about 8 million Ukrainians had been internally displaced and more than 8.2 million had fled the country by April 2023, creating Europe's largest\n",
        "refugee crisis since World War II.\"\"\"\n",
        "\n",
        "summary = textrank_summary(text, num_sentences=3)\n",
        "print(\"Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "307IaXSPK6uM",
        "outputId": "1b563545-0e7e-4826-d6a6-0813a397c3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " The invasion, the largest conflict in Europe since World War II, has caused hundreds of thousands of military casualties and tens\n",
            "of thousands of Ukrainian civilian casualties.From a population of 41 million,\n",
            "about 8 million Ukrainians had been internally displaced and more than 8.2 million had fled the country by April 2023, creating Europe's largest\n",
            "refugee crisis since World War II.On 24 February 2022, Russia invaded Ukraine in a major escalation of the Russo-Ukrainian War, which started in 2014.\n"
          ]
        }
      ]
    }
  ]
}
