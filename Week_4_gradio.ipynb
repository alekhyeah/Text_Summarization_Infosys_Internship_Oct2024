{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBPGifWNwnxl7Ap3RAoBaW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor0327/Text_Summarization_Infosys_Internship_Oct2024/blob/BandariRohith/Week_4_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "TEIU120gY3hE",
        "outputId": "43c9fc61-49bf-494a-ddea-1ed10bdb88ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://093d64997edcefcefb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://093d64997edcefcefb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install sumy --quiet\n",
        "import os\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up Google API Key for LangChain LLM\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('API')\n",
        "\n",
        "# Import necessary libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import heapq\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Define LLM loader function\n",
        "def load_llm(model=\"gemini-1.5-pro\"):\n",
        "    if model == \"gemini-1.5-pro\":\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
        "    elif model == \"gemini-1.5-flash\":\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0, max_tokens=None, timeout=None, max_retries=2)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name\")\n",
        "    return llm\n",
        "\n",
        "# Define prompt template for the LLM\n",
        "def get_prompt_template():\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"Write a concise summary of the following in {num_words} words:\\n\\n\"),\n",
        "            (\"human\", \"{context}\")\n",
        "        ]\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Function to use LLM for summarization with size handling\n",
        "def abstractive_summarization_llm(txt, num_words, model=\"gemini-1.5-pro\"):\n",
        "    try:\n",
        "        llm = load_llm(model)\n",
        "        prompt = get_prompt_template()\n",
        "\n",
        "        # Ensure input text is within acceptable length for the model\n",
        "        if len(txt.split()) > 1024:  # Adjust this number as per your model's limit\n",
        "            txt = ' '.join(txt.split()[:1024])  # Truncate to first 1024 words\n",
        "\n",
        "        chain = prompt | llm\n",
        "        result = chain.invoke({\"context\": txt, \"num_words\": num_words})\n",
        "        summary_text = result.content\n",
        "\n",
        "        return summary_text, len(txt.split()), len(summary_text.split())\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", len(txt.split()), 0\n",
        "\n",
        "# Extractive Summarization Functions\n",
        "\n",
        "def extractive_summarization_frequency(txt, n):\n",
        "    sentences = sent_tokenize(txt)\n",
        "    words = [word.lower() for word in word_tokenize(txt) if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in filtered_words:\n",
        "        word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    max_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies:\n",
        "        word_frequencies[word] /= max_frequency\n",
        "\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.isalpha()]\n",
        "        if len(sentence.split(' ')) < 30:\n",
        "            for word in sentence_words:\n",
        "                if word in word_frequencies:\n",
        "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_frequencies[word]\n",
        "\n",
        "    summary = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "    summary_text = \" \".join(summary)\n",
        "\n",
        "    return summary_text, len(txt.split()), len(summary_text.split())\n",
        "\n",
        "def extractive_summarization_textrank(txt, n):\n",
        "    parser = PlaintextParser.from_string(txt, Tokenizer(\"english\"))\n",
        "    summarizer = TextRankSummarizer()\n",
        "\n",
        "    summary = summarizer(parser.document, n)\n",
        "    summary_text = \" \".join([str(sentence) for sentence in summary])\n",
        "\n",
        "    return summary_text, len(txt.split()), len(summary_text.split())\n",
        "\n",
        "# Abstractive Summarization Functions with Transformers\n",
        "\n",
        "\n",
        "def abstractive_summarization_bart_split(txt, num_words):\n",
        "    # Load the BART model and tokenizer\n",
        "    bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Split text into chunks\n",
        "    inputs = tokenizer(txt, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
        "    input_length = len(inputs['input_ids'][0])\n",
        "\n",
        "    if input_length > 1024:\n",
        "        # Split the text into smaller chunks, each with a maximum of 1024 tokens\n",
        "        tokenized_text = tokenizer.encode(txt)\n",
        "        chunk_size = 1024\n",
        "        chunks = [tokenized_text[i:i + chunk_size] for i in range(0, len(tokenized_text), chunk_size)]\n",
        "\n",
        "        # Summarize each chunk\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "            summary = bart_summarizer(chunk_text, max_length=num_words, min_length=int(num_words * 0.5), do_sample=False)[0]['summary_text']\n",
        "            summaries.append(summary)\n",
        "\n",
        "        # Combine the summaries from each chunk\n",
        "        final_summary = \" \".join(summaries)\n",
        "    else:\n",
        "        # If the text fits within the token limit, summarize it as usual\n",
        "        final_summary = bart_summarizer(txt, max_length=num_words, min_length=int(num_words * 0.5), do_sample=False)[0]['summary_text']\n",
        "\n",
        "    return final_summary, len(txt.split()), len(final_summary.split())\n",
        "\n",
        "\n",
        "def abstractive_summarization_t5(txt, num_words):\n",
        "    t5_summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "    # Tokenize and summarize with T5 (handles larger inputs similarly)\n",
        "    summary = t5_summarizer(txt, max_length=num_words, min_length=int(num_words * 0.5), do_sample=False)[0]['summary_text']\n",
        "\n",
        "    return summary, len(txt.split()), len(summary.split())\n",
        "\n",
        "# Gradio UI setup with gr.Blocks(title=\"Comprehensive Summarizer App\")\n",
        "with gr.Blocks(title=\"Comprehensive Summarizer App\") as demo:\n",
        "    gr.Markdown(\"<h1 style='text-align: center; color: #4A90E2;'>Comprehensive Summarizer App</h1>\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Extractive Summarization Tab\n",
        "        with gr.TabItem(\"Extractive Summarization\"):\n",
        "            input_text = gr.Textbox(label=\"Input Text\", lines=10, placeholder=\"Paste your text here...\")\n",
        "            method = gr.Dropdown(choices=[\"FREQUENCY\", \"TEXT RANK\"], label=\"Select Method\", value=\"FREQUENCY\")\n",
        "            line_limit = gr.Slider(label=\"Summary Line Limit\", minimum=1, maximum=10, step=1, value=2)\n",
        "            output_text = gr.Textbox(label=\"Summary\", lines=5, placeholder=\"Your summary will appear here...\")\n",
        "            word_count = gr.Textbox(label=\"Word Count (Input | Summary)\", interactive=False)\n",
        "\n",
        "            def extractive_summarize_text(text, method, n):\n",
        "                if method == \"FREQUENCY\":\n",
        "                    summary, input_words, summary_words = extractive_summarization_frequency(text, n)\n",
        "                elif method == \"TEXT RANK\":\n",
        "                    summary, input_words, summary_words = extractive_summarization_textrank(text, n)\n",
        "                return summary, f\"{input_words} | {summary_words}\"\n",
        "\n",
        "            gr.Interface(fn=extractive_summarize_text,\n",
        "                         inputs=[input_text, method, line_limit],\n",
        "                         outputs=[output_text, word_count])\n",
        "\n",
        "        # Abstractive Summarization Tab\n",
        "        with gr.TabItem(\"Abstractive Summarization\"):\n",
        "            input_text_abstractive = gr.Textbox(label=\"Input Text\", lines=10,\n",
        "                                                 placeholder=\"Paste your text here...\")\n",
        "            method_abstractive = gr.Dropdown(choices=[\"BART\", \"T5\", \"LLM\"], label=\"Select Method\", value=\"BART\")\n",
        "            word_limit_abstractive = gr.Slider(label=\"Summary Word Limit\", minimum=20,\n",
        "                                                maximum=100,\n",
        "                                                step=10,\n",
        "                                                value=50)\n",
        "            output_text_abstractive = gr.Textbox(label=\"Summary\", lines=5,\n",
        "                                                  placeholder=\"Your summary will appear here...\")\n",
        "            word_count_abstractive = gr.Textbox(label=\"Word Count (Input | Summary)\", interactive=False)\n",
        "\n",
        "            def abstractive_summarize_text(text, method, num_words):\n",
        "                if method == \"BART\":\n",
        "                    summary, input_words, summary_words = abstractive_summarization_bart(text,\n",
        "                                                                                         num_words)\n",
        "                elif method == \"T5\":\n",
        "                    summary, input_words, summary_words = abstractive_summarization_t5(text,\n",
        "                                                                                         num_words)\n",
        "                elif method == \"LLM\":\n",
        "                    summary, input_words, summary_words = abstractive_summarization_llm(text,\n",
        "                                                                                         num_words)\n",
        "                return summary,f\"{input_words} | {summary_words}\"\n",
        "\n",
        "            gr.Interface(fn=abstractive_summarize_text,\n",
        "                         inputs=[input_text_abstractive,\n",
        "                                 method_abstractive,\n",
        "                                 word_limit_abstractive],\n",
        "                         outputs=[output_text_abstractive,\n",
        "                                  word_count_abstractive])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ]
}