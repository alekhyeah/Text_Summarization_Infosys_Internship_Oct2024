{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install sumy --quiet\n",
        "!pip install langchain_google_genai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IoPHKmSe4gf",
        "outputId": "d795b8fa-33e3-442e-f462-1c13b655db32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "FhShLrxCHwWt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF2RAo6aTvZo",
        "outputId": "3e8573d2-31d7-4cc5-927b-597fe7331422"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency-based summarization\n",
        "def frequency_based_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        words = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_scores = {}\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in word_freq:\n",
        "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_freq[word]\n",
        "\n",
        "        sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "\n",
        "        for sentence in sorted_sentences:\n",
        "            word_count += len(word_tokenize(sentence))\n",
        "            if word_count <= max_words:\n",
        "                summary.append(sentence)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LexRank summarization\n",
        "def lexrank_summary(text, max_words):\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        summarizer = LexRankSummarizer()\n",
        "        sentences = parser.document.sentences\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for sentence in summarizer(parser.document, len(sentences)):\n",
        "            sentence_words = len(word_tokenize(str(sentence)))\n",
        "            if word_count + sentence_words <= int(max_words):\n",
        "                summary.append(str(sentence))\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# TextRank summarization\n",
        "def textrank_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        clean_sentences = []\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        for sentence in sentences:\n",
        "            words = word_tokenize(sentence.lower())\n",
        "            words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "            clean_sentences.append(\" \".join(words))\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(graph)\n",
        "        ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for _, sentence in ranked_sentences:\n",
        "            sentence_words = len(word_tokenize(sentence))\n",
        "            if word_count + sentence_words <= max_words:\n",
        "                summary.append(sentence)\n",
        "                word_count += sentence_words\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# T5 summarization\n",
        "def t5_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "        input_text = \"summarize: \" + text\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# BART summarization\n",
        "def bart_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "        inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_words, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# LLM (Gemini) summarization\n",
        "def llm_summary(text, max_words):\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, max_output_tokens=max_words)\n",
        "        prompt = ChatPromptTemplate.from_messages([(\"system\", f\"Summarize this text in {max_words} words:\\n\\n\"), (\"human\", text)])\n",
        "        chain = prompt | llm\n",
        "        result = chain.invoke({\"text\": text})\n",
        "        return result.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Extractive summarization function\n",
        "def extractive_summarize(text, method, max_words):\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to summarize.\"\n",
        "\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        if max_words < 1:\n",
        "            return \"Please enter a positive number of words.\"\n",
        "\n",
        "        if method == \"Frequency-based\":\n",
        "            return frequency_based_summary(text, max_words)\n",
        "        elif method == \"LexRank\":\n",
        "            return lexrank_summary(text, max_words)\n",
        "        elif method == \"TextRank\":\n",
        "            return textrank_summary(text, max_words)\n",
        "        return \"Invalid method selected\"\n",
        "    except ValueError:\n",
        "        return \"Please enter a valid number for maximum words.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Abstractive summarization function\n",
        "def abstractive_summarize(text, method, max_words):\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to summarize.\"\n",
        "\n",
        "    try:\n",
        "        max_words = int(max_words)\n",
        "        if max_words < 1:\n",
        "            return \"Please enter a positive number of words.\"\n",
        "\n",
        "        if method == \"T5\":\n",
        "            return t5_summary(text, max_words)\n",
        "        elif method == \"BART\":\n",
        "            return bart_summary(text, max_words)\n",
        "        elif method == \"LLM (Gemini)\":\n",
        "            return llm_summary(text, max_words)\n",
        "        return \"Invalid method selected\"\n",
        "    except ValueError:\n",
        "        return \"Please enter a valid number for maximum words.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Text Summarization App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Extractive Summarization\"):\n",
        "            method = gr.Dropdown(choices=[\"Frequency-based\", \"LexRank\", \"TextRank\"], label=\"Summarization Method\")\n",
        "            words = gr.Textbox(label=\"Maximum Words\", value=\"100\")\n",
        "            text = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter text here\")\n",
        "            output = gr.Textbox(label=\"Summary\", lines=5)\n",
        "            button = gr.Button(\"Generate Summary\")\n",
        "            button.click(extractive_summarize, inputs=[text, method, words], outputs=output)\n",
        "\n",
        "        with gr.Tab(\"Abstractive Summarization\"):\n",
        "            method = gr.Dropdown(choices=[\"T5\", \"BART\", \"LLM (Gemini)\"], label=\"Summarization Method\")\n",
        "            words = gr.Textbox(label=\"Maximum Words\", value=\"100\")\n",
        "            text = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter text here\")\n",
        "            output = gr.Textbox(label=\"Summary\", lines=5)\n",
        "            button = gr.Button(\"Generate Summary\")\n",
        "            button.click(abstractive_summarize, inputs=[text, method, words], outputs=output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "hRoG1dNQ2b8Q",
        "outputId": "59e94cba-7faa-4cea-88f1-27a15607ef45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f211b72f5108e5a6de.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f211b72f5108e5a6de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbyFVPQF2e3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
