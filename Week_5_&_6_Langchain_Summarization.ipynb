{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQyJorGst3Zl7vQxC2ESlN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor0327/Text_Summarization_Infosys_Internship_Oct2024/blob/BandariRohith/Week_5_%26_6_Langchain_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing required packagesand setting up environment"
      ],
      "metadata": {
        "id": "OlZmYh1XlYfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken openai langchain --quiet\n",
        "!pip install langchain.community --quiet\n",
        "!pip install langchain_google_genai --quiet\n",
        "!pip install -qU langchain-google-vertexai --quiet\n",
        "!pip install --upgrade --quiet langchain langchain-google-genai beautifulsoup4\n",
        "!pip install -qU langgraph\n",
        "!pip install pypdf --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBlU2Pr7leMZ",
        "outputId": "0aeb8145-9b8c-488e-b4b5-0e5ce69d9022"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('API')\n",
        "#for single cell\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.chains import ReduceDocumentsChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "#for map reduce\n",
        "import textwrap\n",
        "import operator\n",
        "from typing import List, Literal, TypedDict\n",
        "from google.colab import userdata\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.constants import Send\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "aOogf84Nn_If"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0,max_tokens=None,timeout=None,max_retries=2)\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
        "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
        "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
        "]\n",
        "\n",
        "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\",\n",
        "                             verbose = False)\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGm0m8MCokjp",
        "outputId": "1b6d87b3-6689-41ef-d69e-2f3b709b5fde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ed35e8a2ebe7>:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  output_summary = chain.run(docs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fruits have various colors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Cell LangChain"
      ],
      "metadata": {
        "id": "GHKutHQ_llh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SINGLE CELL LANGCHAIN\n",
        "\n",
        "\n",
        "!pip install -qU langchain-google-vertexai --quiet\n",
        "!pip install --upgrade --quiet langchain langchain-google-genai beautifulsoup4\n",
        "\n",
        "\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0,max_tokens=None,timeout=None,max_retries=2)\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
        "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
        "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
        "]\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Summarize this content: {context}\")\n",
        "chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "result = chain.invoke({\"context\": documents})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlgZ-cstlpWb",
        "outputId": "9ee2d012-2020-4c5b-9ac7-0b5eeca156c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text describes the colors of three fruits: apples (red), blueberries (blue), and bananas (yellow).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Iterative Refinement"
      ],
      "metadata": {
        "id": "MLLJcUvYltKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ITERATIVE REFINEMENT\n",
        "!pip install -qU langchain-google-vertexai --quiet\n",
        "!pip install --upgrade --quiet langchain langchain-google-genai beautifulsoup4\n",
        "!pip install -qU langgraph\n",
        "\n",
        "import operator\n",
        "from typing import List, Literal, TypedDict\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.constants import Send\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"API\")\n",
        "\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0,max_tokens=None,timeout=None,max_retries=2)\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
        "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
        "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
        "]\n",
        "\n",
        "summarize_prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
        "    ]\n",
        ")\n",
        "initial_summary_chain = summarize_prompt | llm | StrOutputParser()\n",
        "\n",
        "refine_template = \"\"\"\n",
        "Produce a final summary.\n",
        "\n",
        "Existing summary up to this point:\n",
        "{existing_answer}\n",
        "\n",
        "New context:\n",
        "------------\n",
        "{context}\n",
        "------------\n",
        "\n",
        "Given the new context, refine the original summary.\n",
        "\"\"\"\n",
        "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
        "\n",
        "refine_summary_chain = refine_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# We will define the state of the graph to hold the document\n",
        "# contents and summary. We also include an index to keep track\n",
        "# of our position in the sequence of documents.\n",
        "class State(TypedDict):\n",
        "    contents: List[str]\n",
        "    index: int\n",
        "    summary: str\n",
        "\n",
        "\n",
        "# We define functions for each node, including a node that generates\n",
        "# the initial summary:\n",
        "async def generate_initial_summary(state: State, config: RunnableConfig):\n",
        "    summary = await initial_summary_chain.ainvoke(\n",
        "        state[\"contents\"][0],\n",
        "        config,\n",
        "    )\n",
        "    return {\"summary\": summary, \"index\": 1}\n",
        "\n",
        "\n",
        "# And a node that refines the summary based on the next document\n",
        "async def refine_summary(state: State, config: RunnableConfig):\n",
        "    content = state[\"contents\"][state[\"index\"]]\n",
        "    summary = await refine_summary_chain.ainvoke(\n",
        "        {\"existing_answer\": state[\"summary\"], \"context\": content},\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    return {\"summary\": summary, \"index\": state[\"index\"] + 1}\n",
        "\n",
        "\n",
        "# Here we implement logic to either exit the application or refine\n",
        "# the summary.\n",
        "def should_refine(state: State) -> Literal[\"refine_summary\", END]:\n",
        "    if state[\"index\"] >= len(state[\"contents\"]):\n",
        "        return END\n",
        "    else:\n",
        "        return \"refine_summary\"\n",
        "\n",
        "\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"generate_initial_summary\", generate_initial_summary)\n",
        "graph.add_node(\"refine_summary\", refine_summary)\n",
        "\n",
        "graph.add_edge(START, \"generate_initial_summary\")\n",
        "graph.add_conditional_edges(\"generate_initial_summary\", should_refine)\n",
        "graph.add_conditional_edges(\"refine_summary\", should_refine)\n",
        "app = graph.compile()\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(app.get_graph().draw_mermaid_png())\n",
        "\n",
        "async for step in app.astream(\n",
        "    {\"contents\": [doc.page_content for doc in documents]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    if summary := step.get(\"summary\"):\n",
        "        print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb-ph2YFlvtw",
        "outputId": "b56c8dc5-cdb8-4442-99c8-ae4bbba80fc7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apples can be red.\n",
            "\n",
            "Apples can be red, and blueberries are blue.\n",
            "\n",
            "Apples are red, blueberries are blue, and bananas are yellow.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map-Reduce Summarization"
      ],
      "metadata": {
        "id": "-fAfc3b9oQ-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0,max_tokens=None,timeout=None,max_retries=2)\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
        "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
        "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
        "]\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "map_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"human\", \"Write a concise summary of the following:\\n\\n{context}\")]\n",
        ")\n",
        "\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "reduce_template = \"\"\"\n",
        "The following is a set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary\n",
        "of the main themes.\n",
        "\"\"\"\n",
        "\n",
        "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "from langchain.chains.combine_documents.reduce import (\n",
        "    acollapse_docs,\n",
        "    split_list_of_docs,\n",
        ")\n",
        "\n",
        "token_max = 1000\n",
        "\n",
        "def length_function(documents: List[Document]) -> int:\n",
        "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
        "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)\n",
        "\n",
        "class OverallState(TypedDict):\n",
        "    # Notice here we use the operator.add\n",
        "    # This is because we want combine all the summaries we generate\n",
        "    # from individual nodes back into one list - this is essentially\n",
        "    # the \"reduce\" part\n",
        "    contents: List[str]\n",
        "    summaries: Annotated[list, operator.add]\n",
        "    collapsed_summaries: List[Document]\n",
        "    final_summary: str\n",
        "\n",
        "\n",
        "# This will be the state of the node that we will \"map\" all\n",
        "# documents to in order to generate summaries\n",
        "class SummaryState(TypedDict):\n",
        "    content: str\n",
        "\n",
        "\n",
        "# Here we generate a summary, given a document\n",
        "async def generate_summary(state: SummaryState):\n",
        "    response = await map_chain.ainvoke(state[\"content\"])\n",
        "    return {\"summaries\": [response]}\n",
        "\n",
        "def map_summaries(state: OverallState):\n",
        "    return [\n",
        "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "def collect_summaries(state: OverallState):\n",
        "    return {\n",
        "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
        "    }\n",
        "\n",
        "\n",
        "# Add node to collapse summaries\n",
        "async def collapse_summaries(state: OverallState):\n",
        "    doc_lists = split_list_of_docs(\n",
        "        state[\"collapsed_summaries\"], length_function, token_max\n",
        "    )\n",
        "    results = []\n",
        "    for doc_list in doc_lists:\n",
        "        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))\n",
        "\n",
        "    return {\"collapsed_summaries\": results}\n",
        "\n",
        "\n",
        "# This represents a conditional edge in the graph that determines\n",
        "# if we should collapse the summaries or not\n",
        "def should_collapse(\n",
        "    state: OverallState,\n",
        ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
        "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
        "    if num_tokens > token_max:\n",
        "        return \"collapse_summaries\"\n",
        "    else:\n",
        "        return \"generate_final_summary\"\n",
        "\n",
        "async def generate_final_summary(state: OverallState):\n",
        "    response = await reduce_chain.ainvoke(state[\"collapsed_summaries\"])\n",
        "    return {\"final_summary\": response}\n",
        "\n",
        "# Construct the graph\n",
        "# Nodes:\n",
        "graph = StateGraph(OverallState)\n",
        "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
        "graph.add_node(\"collect_summaries\", collect_summaries)\n",
        "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
        "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
        "\n",
        "# Edges:\n",
        "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
        "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
        "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
        "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
        "graph.add_edge(\"generate_final_summary\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "from IPython.display import Image\n",
        "Image(app.get_graph().draw_mermaid_png())\n",
        "\n",
        "async for step in app.astream(\n",
        "    {\"contents\": [doc.page_content for doc in documents]},\n",
        "    {\"recursion_limit\": 20},\n",
        "):\n",
        "    print(list(step.keys()))\n",
        "\n",
        "print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD2kRTGcoR1c",
        "outputId": "e415f593-460a-427e-959e-ef41bb635834"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['generate_summary']\n",
            "['generate_summary']\n",
            "['generate_summary']\n",
            "['collect_summaries']\n",
            "['generate_final_summary']\n",
            "{'generate_final_summary': {'final_summary': 'Fruits come in various colors.\\n'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF summary"
      ],
      "metadata": {
        "id": "69wnO21QqciL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PDF Summary\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "      model=\"gemini-1.5-flash\",\n",
        "      temperature=0,\n",
        "      max_tokens=None,\n",
        "      timeout=None,\n",
        "      max_retries=2\n",
        ")\n",
        "\n",
        "def summarize_pdf(pdf_file_path):\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs = loader.load_and_split()\n",
        "    chain = load_summarize_chain(llm, chain_type=\"refine\") #map_reduce\n",
        "    summary = chain.invoke(docs)\n",
        "\n",
        "    return summary['output_text']\n",
        "\n",
        "summary = summarize_pdf(\"/content/Vanka-By-Anton-Chekhov-book-PDF.pdf\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UHo5X87oTVi",
        "outputId": "2581d78f-98f7-4114-c74a-960511597664"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nine-year-old Vanka Zhukov, an orphaned apprentice to a shoemaker in Moscow, writes a desperate letter to his grandfather, Konstantin Makaritch, on Christmas Eve, secretly using his master's supplies.  He vividly describes his miserable existence—hunger, overwork, physical and verbal abuse, and sleep deprivation—contrasting it with cherished memories of his happy childhood in the village with his grandfather.  These memories, including gathering the Christmas tree and the warmth of his grandfather's home, highlight the stark difference between his past and present.  Before his mother's death, Vanka lived with his grandfather and received kindness from Olga Ignatyevna. He pleads with his grandfather for rescue, offering to work tirelessly in return.  Glimpses of Moscow's opulence only emphasize Vanka's poverty and isolation.  He concludes by requesting a gilt walnut from a nearby grand house's Christmas tree, symbolizing the love and warmth he craves.  After completing the letter, Vanka, fueled by hope, delivers it to a postbox, his naive understanding of the postal system adding to the poignancy of his situation. He falls asleep dreaming of his grandfather reading the letter and the comforting presence of his dog, Eel, by the stove, a peaceful contrast to his harsh reality.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}