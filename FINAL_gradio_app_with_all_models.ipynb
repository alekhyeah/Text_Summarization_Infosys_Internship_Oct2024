{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor0327/Text_Summarization_Infosys_Internship_Oct2024/blob/Sameer/FINAL_gradio_app_with_all_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K2DRGIzkNmy6",
        "outputId": "8d7d5ec9-5557-4953-a401-38702d139e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2IzcpLoyHhK"
      },
      "source": [
        "**FREQUENCY MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FrSiCmrqx0jZ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7GFgjBUzyGke",
        "outputId": "caaa08a2-c505-448c-fa7c-a594ef5b345d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "G7w-0v3eyUst"
      },
      "outputs": [],
      "source": [
        "def freq_model(text, max_words=30):\n",
        "    # Load English stopwords\n",
        "    stopwords1 = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Create a frequency table for words (ignoring stopwords)\n",
        "    freqTable = {word.lower(): words.count(word.lower()) for word in words if word.lower() not in stopwords1}\n",
        "\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Create a sentence value dictionary and sentence score\n",
        "    sentenceValue = {}\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                sentenceValue[sentence] = sentenceValue.get(sentence, 0) + freq\n",
        "\n",
        "    # Calculate the sum of sentence values\n",
        "    sumValues = sum(sentenceValue.values())\n",
        "\n",
        "    # Calculate the average sentence value\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "    # Generate the summary with max and min word constraints\n",
        "    summary_sentences = []\n",
        "    current_word_count = 0\n",
        "    min_words = max_words - 10  # Default minimum words limit\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentenceValue.get(sentence, 0) > (1.2 * average):\n",
        "            sentence_word_count = len(word_tokenize(sentence))\n",
        "            if min_words <= current_word_count + sentence_word_count <= max_words:\n",
        "                summary_sentences.append(sentence)\n",
        "                current_word_count += sentence_word_count\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    summary_word_count = len(word_tokenize(summary))\n",
        "    return summary,(f\"\\nWord Count (Summary): {summary_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtBXuXJ3VFtS"
      },
      "source": [
        "**SUMY MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5fxVNw97VMmV",
        "outputId": "c1498fd4-0156-42c5-a675-89324eafc3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "MDao_62FVQCA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AWmsgdJ7VY0c"
      },
      "outputs": [],
      "source": [
        "def sumy_model(text, max_words=50):\n",
        "    # Create a plaintext parser with English tokenization\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Create a TextRank summarizer\n",
        "    summarizer = TextRankSummarizer()\n",
        "\n",
        "    # Summarize the text, initially setting to retrieve more sentences than needed\n",
        "    summary_sentences = summarizer(parser.document, len(parser.document.sentences))\n",
        "\n",
        "    # Initialize variables for word count\n",
        "    summary = []\n",
        "    current_word_count = 0\n",
        "    min_words = max_words - 10  # Default minimum word count\n",
        "\n",
        "    # Iterate over sentences, adding them until we reach the max word limit\n",
        "    for sentence in summary_sentences:\n",
        "        sentence_word_count = len(word_tokenize(str(sentence)))\n",
        "        if current_word_count + sentence_word_count > max_words:\n",
        "            break  # Stop if adding the sentence exceeds max_words\n",
        "        if current_word_count + sentence_word_count >= min_words:\n",
        "            summary.append(str(sentence))\n",
        "            current_word_count += sentence_word_count\n",
        "\n",
        "    # Convert the summary list to a string\n",
        "    text_summary = \" \".join(summary)\n",
        "    summary_word_count = len(word_tokenize(text_summary))\n",
        "    return text_summary,(f\"\\nWord Count (Summary): {summary_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF5BouQiXW9W"
      },
      "source": [
        "**LUHN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "collapsed": true,
        "id": "VqlHkZlkXaXH"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "SJrsLL6oXkCo"
      },
      "outputs": [],
      "source": [
        "def luhn_model(text, max_words=30):\n",
        "    # Create a plaintext parser with English tokenization\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Create a Luhn summarizer\n",
        "    summarizer_luhn = LuhnSummarizer()\n",
        "\n",
        "    # Generate all sentences in the summary initially\n",
        "    summary_sentences = summarizer_luhn(parser.document, len(parser.document.sentences))\n",
        "\n",
        "    # Initialize variables for word count\n",
        "    summary = []\n",
        "    current_word_count = 0\n",
        "    min_words = max(0, max_words - 10)  # Default minimum word count, ensures it’s non-negative\n",
        "\n",
        "    # Iterate over sentences, adding them until we reach the max word limit\n",
        "    for sentence in summary_sentences:\n",
        "        sentence_word_count = len(word_tokenize(str(sentence)))\n",
        "\n",
        "        # Check if adding this sentence would exceed max_words\n",
        "        if current_word_count + sentence_word_count > max_words:\n",
        "            break  # Stop if adding the sentence would exceed max_words\n",
        "\n",
        "        # Add the sentence to summary and update the word count\n",
        "        summary.append(str(sentence))\n",
        "        current_word_count += sentence_word_count\n",
        "\n",
        "        # Break if we’ve reached the minimum word count required and are close to max_words\n",
        "        if current_word_count >= min_words:\n",
        "            break\n",
        "\n",
        "    # Convert the summary list to a single string\n",
        "    final_summary = ' '.join(summary)\n",
        "    summary_word_count = len(word_tokenize(final_summary))\n",
        "    return final_summary,(f\"\\nWord Count (Summary): {summary_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EorWCRSqYwUp"
      },
      "source": [
        "**T5 MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EiJhGTh4Yv6y"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import glob\n",
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "EeuxnoK4Y0mr"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ELmXTUdzY58h"
      },
      "outputs": [],
      "source": [
        "def t5_model(text,max_words):\n",
        "    num_beams=5\n",
        "    # Define min and max words for generation based on the provided max_words\n",
        "    min_words = max(1, max_words - 10)\n",
        "    max_tokens = max_words * 1.5  # Set max tokens with some buffer for BERT-based tokenization\n",
        "\n",
        "    # Initialize the generated summary\n",
        "    summary = \"\"\n",
        "\n",
        "    # Repeat generation to ensure summary falls within the word count range\n",
        "    while True:\n",
        "        # Preprocess the text for encoding\n",
        "        inputs = tokenizer.encode(\n",
        "            \"summarize: \" + text,\n",
        "            return_tensors='pt',\n",
        "            max_length=512,  # Limit input length to avoid truncation\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Generate the summary with length constraints\n",
        "        summary_ids = model.generate(\n",
        "            inputs,\n",
        "            max_length=int(max_tokens),     # Set max tokens for output\n",
        "            min_length=min_words,           # Set min length\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode and calculate the word count of the generated summary\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summary_words = summary.split()\n",
        "\n",
        "        # Check if the summary is within the desired word range\n",
        "        word_count = len(summary_words)\n",
        "        if min_words <= word_count <= max_words:\n",
        "            break\n",
        "\n",
        "        # Adjust max_tokens slightly to retry if the summary doesn’t fit within the range\n",
        "        max_tokens -= 5\n",
        "        if max_tokens < min_words * 1.5:\n",
        "            break  # Stop if max_tokens becomes impractically low\n",
        "    summary_word_count = len(summary_words)\n",
        "    return summary,(f\"\\nWord Count (Summary): {summary_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwc7Tln1dvF1"
      },
      "source": [
        "**LLM MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_E8v_iuadzhZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "collapsed": true,
        "id": "YGwok2pSjrS-"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain langchain-google-genai langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "YvOPr0cPjuSJ"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def load_llm(model=\"gemini-1.5-pro\"):\n",
        "\n",
        "  if model == \"gemini-1.5-pro\":\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2)\n",
        "    return llm\n",
        "  elif model == \"gemini-1.5-flash\":\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2)\n",
        "    return llm\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9o8uzD05jy-e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def get_prompt_template():\n",
        "    # Define prompt with a strict word range instruction\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Write a concise summary of the following in {min_word_count} to {max_word_count} words. Stop once you reach the word limit:\\n\\n\",\n",
        "            ),\n",
        "            (\"human\", \"{context}\")\n",
        "        ]\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def llm_model(text, max_word_count=50):\n",
        "    model=\"gemini-1.5-pro\"\n",
        "    # Set default for min_word_count if not provided\n",
        "    # if min_word_count is None:\n",
        "    min_word_count = max_word_count - 5\n",
        "\n",
        "    # Load LLM\n",
        "    llm = load_llm(model)\n",
        "\n",
        "    # Get Prompt Template\n",
        "    prompt = get_prompt_template()\n",
        "\n",
        "    # Instantiate chain\n",
        "    chain = prompt | llm\n",
        "\n",
        "    # Invoke chain with specified range\n",
        "    result = chain.invoke({\n",
        "        \"context\": text,\n",
        "        \"min_word_count\": min_word_count,\n",
        "        \"max_word_count\": max_word_count\n",
        "    })\n",
        "\n",
        "    # Process output to enforce word limit\n",
        "    output = result.content\n",
        "    words = output.split()\n",
        "\n",
        "    # If output exceeds max_word_count, truncate to max_word_count\n",
        "    if len(words) > max_word_count:\n",
        "        output = ' '.join(words[:max_word_count]) + \".\"\n",
        "\n",
        "    # Return truncated output if necessary\n",
        "    return output,(f\"\\nWord Count (Summary): {len(output.split())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LrI7SY0pRcj"
      },
      "source": [
        "**BART MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "QrOIDmG2svmS"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WlkQTqHrwJhn"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "def bart_model(article, max_words=50):\n",
        "    # Load BART model and tokenizer\n",
        "    model_name = 'facebook/bart-large-cnn'\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Define minimum and maximum words for the summary\n",
        "    min_words = max(10, max_words - 10)\n",
        "    max_tokens = max_words * 1.5  # Adding a buffer as BERT-based tokens don't align 1:1 with words\n",
        "\n",
        "    # Count the number of words in the original article\n",
        "    original_word_count = len(article.split())\n",
        "    print(f\"Original article word count: {original_word_count}\")\n",
        "\n",
        "    # Tokenize and encode the article\n",
        "    inputs = tokenizer.encode(article, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary with specified length constraints\n",
        "    summary = \"\"\n",
        "    while True:\n",
        "        summary_ids = model.generate(\n",
        "            inputs,\n",
        "            num_beams=4,\n",
        "            min_length=min_words,           # Set minimum tokens\n",
        "            max_length=int(max_tokens),      # Set maximum tokens\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode and calculate word count of the summary\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summary_word_count = len(summary.split())\n",
        "\n",
        "        # Check if the summary is within the specified range\n",
        "        if min_words <= summary_word_count <= max_words:\n",
        "            break\n",
        "\n",
        "        # Reduce max_tokens slightly and retry if summary is too long\n",
        "        max_tokens -= 5\n",
        "        if max_tokens < min_words * 1.5:  # Stop if max_tokens is too low to avoid an infinite loop\n",
        "            break\n",
        "    return summary,(f\"Generated summary word count: {summary_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcFE6w6Xx0Gm"
      },
      "source": [
        "**Defining functions of all models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "vnih-oi2fVMg"
      },
      "outputs": [],
      "source": [
        "# Placeholder for input and method-specific functions\n",
        "def extractive_summarize(text, method, max_words):\n",
        "    if method == \"Frequency Method\":\n",
        "        return freq_model(text, max_words)\n",
        "    elif method == \"Sumy Method\":\n",
        "        return sumy_model(text, max_words)\n",
        "    elif method == \"Luhn Method\":\n",
        "        return luhn_model(text, max_words)\n",
        "\n",
        "def abstractive_summarize(text, model, max_words):\n",
        "    if model == \"T5\":\n",
        "        return t5_model(text,max_words)\n",
        "    elif model == \"BART\":\n",
        "        return bart_model(text, max_words)\n",
        "\n",
        "\n",
        "def advance_summarize(text, model, max_words):\n",
        "    if model == \"LLM Model\":\n",
        "        return llm_model(text, max_words)\n",
        "    elif model == \"refine\":\n",
        "        return llm_model(text, max_words)\n",
        "    elif model == \"map_Reduce\":\n",
        "        return llm_model(text, max_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Extractor**"
      ],
      "metadata": {
        "id": "0FAAwXnfBpuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "noTfnYoYQKUY",
        "outputId": "b139cc3b-fca3-46f9-8b60-341aede98eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "nQsAiYUEP9Pf"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extract text from uploaded PDF file.\"\"\"\n",
        "    reader = PdfReader(pdf_file.name)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy2M5JQFZBBD"
      },
      "source": [
        "**Document Summarizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "collapsed": true,
        "id": "iO9Ic9RfZAnw"
      },
      "outputs": [],
      "source": [
        "!pip install -U gradio langchain langchain-community pypdf langchain-google-genai langgraph --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "SlQYyxZzZMGu"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "BXgZDUkwZQHq"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "      model=\"gemini-1.5-flash\",\n",
        "      temperature=0,\n",
        "      max_tokens=None,\n",
        "      timeout=None,\n",
        "      max_retries=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "lZ5-Tu3gZRBy"
      },
      "outputs": [],
      "source": [
        "def summarize_pdf(pdf_file_path, chain_type, max_word_count):\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs = loader.load_and_split()\n",
        "\n",
        "    # Set minimum word count\n",
        "    min_word_count = max_word_count - 10\n",
        "\n",
        "    if chain_type == \"map_Reduce\":\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "        summary = chain.invoke(docs)\n",
        "    elif chain_type == \"refine\":\n",
        "        chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "        summary = chain.invoke(docs)\n",
        "    else:\n",
        "        return \"Enter valid chain_type!! (map_reduce or refine)\"\n",
        "\n",
        "    # Ensure the summary is within the specified word count range\n",
        "    summary_text = summary['output_text']\n",
        "    words = summary_text.split()\n",
        "    word_count = len(words)\n",
        "\n",
        "    if word_count < min_word_count:\n",
        "        # Expand the summary until it reaches the minimum word count\n",
        "        while word_count < min_word_count:\n",
        "            summary_text += \" \" + words[word_count % len(words)]\n",
        "            words = summary_text.split()\n",
        "            word_count = len(words)\n",
        "    elif word_count > max_word_count:\n",
        "        # Truncate the summary until it reaches the maximum word count\n",
        "        summary_text = \" \".join(words[:max_word_count])+'.'\n",
        "        word_count = max_word_count  # Update word count to max_word_count\n",
        "\n",
        "    return summary_text, (f\"\\nWord Count (Summary): {len(summary_text.split())}\")  # Return both summary and word count\n",
        "\n",
        "    # return summary_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Functions**"
      ],
      "metadata": {
        "id": "X44I_CzqBeIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HCOR4Py-QEOf"
      },
      "outputs": [],
      "source": [
        "def extractive_handler(pdf_file, input_text, method, word_count):\n",
        "    text = input_text\n",
        "    if pdf_file is not None:\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "    return extractive_summarize(text, method, word_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def abstractive_handler(pdf_file, input_text, method, word_count):\n",
        "    text = input_text\n",
        "    if pdf_file is not None:\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "    return abstractive_summarize(text, method, word_count)"
      ],
      "metadata": {
        "id": "GFcaEINmw5Uc"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "BYdQCXooRlVD"
      },
      "outputs": [],
      "source": [
        "def advance_handler(pdf_file, input_text, method, max_words):\n",
        "    text = input_text\n",
        "    if pdf_file is not None:\n",
        "        if method == \"map_Reduce\":\n",
        "            return summarize_pdf(pdf_file,method, max_words)\n",
        "        elif method == \"refine\":\n",
        "            return summarize_pdf(pdf_file,method, max_words)\n",
        "        else:\n",
        "            text = extract_text_from_pdf(pdf_file)\n",
        "    return advance_summarize(text, method, max_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio App**"
      ],
      "metadata": {
        "id": "QA0rvEv1CbMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define CSS\n",
        "css = \"\"\"\n",
        "h1 {\n",
        "    margin-top: 2rem;\n",
        "    font-size: 2rem;\n",
        "    text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "input_text = gr.Textbox(label=\"Input Text\", lines=10)\n",
        "max_words = gr.Slider(label=\"Max Words\", minimum=10, maximum=100, step=10, value=50)\n",
        "input_pdf_path = gr.File(label=\"Enter the PDF file path\")\n",
        "# chain_type = gr.Text(label=\"Enter Chain_type\")\n",
        "output_summary = gr.Text(label=\"Summary\")\n",
        "\n",
        "\n",
        "# Function to toggle input fields\n",
        "def toggle_input(input_type):\n",
        "    if input_type == \"Text\":\n",
        "        return gr.update(visible=True), gr.update(visible=False)\n",
        "    elif input_type == \"PDF\":\n",
        "        return gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "# Gradio app structure\n",
        "with gr.Blocks(title=\"Summarizer App\", css=css) as demo:\n",
        "    gr.Markdown(\"# Summarizer App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Extractive\"):\n",
        "            with gr.Row():\n",
        "                # Left column: Inputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Inputs\")\n",
        "                    input_type = gr.Radio(\n",
        "                        [\"Text\", \"PDF\"],\n",
        "                        label=\"Select Input Type\",\n",
        "                        value=\"Text\",\n",
        "                    )\n",
        "\n",
        "                    input_text = gr.Textbox(\n",
        "                        label=\"Input Text\",\n",
        "                        lines=10,\n",
        "                        visible=True,\n",
        "                    )\n",
        "                    input_pdf_path = gr.File(\n",
        "                        label=\"Upload PDF\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        visible=False,\n",
        "                    )\n",
        "\n",
        "                    extractive_dropdown = gr.Dropdown(\n",
        "                        [\"Frequency Method\", \"Sumy Method\", \"Luhn Method\"],\n",
        "                        label=\"Choose Extractive Method\",\n",
        "                    )\n",
        "\n",
        "                    max_words = gr.Slider(\n",
        "                        label=\"Max Words\", minimum=10, maximum=100, step=10, value=40\n",
        "                    )\n",
        "\n",
        "                    summarize_button = gr.Button(\"Summarize\")\n",
        "\n",
        "                # Right column: Outputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Outputs\")\n",
        "                    output_summary = gr.Textbox(label=\"Summary Output\")\n",
        "                    output_word_count = gr.Textbox(label=\"Word Count\")\n",
        "\n",
        "            # Change visibility based on input type selection\n",
        "            input_type.change(\n",
        "                toggle_input,\n",
        "                inputs=input_type,\n",
        "                outputs=[input_text, input_pdf_path],\n",
        "            )\n",
        "\n",
        "            # Trigger summarization\n",
        "            summarize_button.click(\n",
        "                extractive_handler,\n",
        "                inputs=[input_pdf_path,input_text, extractive_dropdown, max_words],\n",
        "                outputs=[output_summary, output_word_count],\n",
        "            )\n",
        "        with gr.TabItem(\"Abstractive\"):\n",
        "            with gr.Row():\n",
        "                # Left column: Inputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Inputs\")\n",
        "                    input_type = gr.Radio(\n",
        "                        [\"Text\", \"PDF\"],\n",
        "                        label=\"Select Input Type\",\n",
        "                        value=\"Text\",\n",
        "                    )\n",
        "\n",
        "                    input_text = gr.Textbox(\n",
        "                        label=\"Input Text\",\n",
        "                        lines=10,\n",
        "                        visible=True,\n",
        "                    )\n",
        "                    input_pdf_path = gr.File(\n",
        "                        label=\"Upload PDF\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        visible=False,\n",
        "                    )\n",
        "\n",
        "                    extractive_dropdown = gr.Dropdown(\n",
        "                        [\"T5\", \"BART\"],\n",
        "                        label=\"Choose Extractive Method\",\n",
        "                    )\n",
        "\n",
        "                    max_words = gr.Slider(\n",
        "                        label=\"Max Words\", minimum=10, maximum=100, step=10, value=40\n",
        "                    )\n",
        "\n",
        "                    summarize_button = gr.Button(\"Summarize\")\n",
        "\n",
        "                # Right column: Outputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Outputs\")\n",
        "                    output_summary = gr.Textbox(label=\"Summary Output\")\n",
        "                    output_word_count = gr.Textbox(label=\"Word Count\")\n",
        "\n",
        "            # Change visibility based on input type selection\n",
        "            input_type.change(\n",
        "                toggle_input,\n",
        "                inputs=input_type,\n",
        "                outputs=[input_text, input_pdf_path],\n",
        "            )\n",
        "\n",
        "            # Trigger summarization\n",
        "            summarize_button.click(\n",
        "                abstractive_handler,\n",
        "                inputs=[input_pdf_path,input_text, extractive_dropdown, max_words],\n",
        "                outputs=[output_summary, output_word_count],\n",
        "            )\n",
        "        with gr.TabItem(\"Advance LLM's\"):\n",
        "            with gr.Row():\n",
        "                # Left column: Inputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Inputs\")\n",
        "                    input_type = gr.Radio(\n",
        "                        [\"Text\", \"PDF\"],\n",
        "                        label=\"Select Input Type\",\n",
        "                        value=\"Text\",\n",
        "                    )\n",
        "\n",
        "                    input_text = gr.Textbox(\n",
        "                        label=\"Input Text\",\n",
        "                        lines=10,\n",
        "                        visible=True,\n",
        "                    )\n",
        "                    input_pdf_path = gr.File(\n",
        "                        label=\"Upload PDF\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        visible=False,\n",
        "                    )\n",
        "\n",
        "                    extractive_dropdown = gr.Dropdown(\n",
        "                        [\"LLM Model\",\"refine\",\"map_Reduce\"],\n",
        "                        label=\"Choose Extractive Method\",\n",
        "                    )\n",
        "\n",
        "                    max_words = gr.Slider(\n",
        "                        label=\"Max Words\", minimum=10, maximum=100, step=10, value=40\n",
        "                    )\n",
        "\n",
        "                    summarize_button = gr.Button(\"Summarize\")\n",
        "\n",
        "                # Right column: Outputs\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### Outputs\")\n",
        "                    output_summary = gr.Textbox(label=\"Summary Output\")\n",
        "                    output_word_count = gr.Textbox(label=\"Word Count\")\n",
        "\n",
        "            # Change visibility based on input type selection\n",
        "            input_type.change(\n",
        "                toggle_input,\n",
        "                inputs=input_type,\n",
        "                outputs=[input_text, input_pdf_path],\n",
        "            )\n",
        "\n",
        "            # Trigger summarization\n",
        "            summarize_button.click(\n",
        "                advance_handler,\n",
        "                inputs=[input_pdf_path,input_text, extractive_dropdown, max_words],\n",
        "                outputs=[output_summary, output_word_count],\n",
        "            )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "3XbFu6LqwBdG",
        "outputId": "c9473466-1894-45d2-a5f9-44520acb8c99"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ad3c1f70462d0167a2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ad3c1f70462d0167a2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TNlUcdSx6iM"
      },
      "source": [
        "**GRADIO APP without pdf version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "7hQwkkNExjNh",
        "collapsed": true,
        "outputId": "fdb0ba5d-65ca-486f-c8f5-a7a9abd98d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://32fd7eede5f6584f5c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://32fd7eede5f6584f5c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://398bfb5f005e57f8a6.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://47d09e27313f8b35b6.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://1aeaac70e42cab9d0c.gradio.live\n",
            "Killing tunnel 127.0.0.1:7863 <> https://8f4afd0fcf5843f558.gradio.live\n",
            "Killing tunnel 127.0.0.1:7864 <> https://e84aff949ed92b0c7b.gradio.live\n",
            "Killing tunnel 127.0.0.1:7865 <> https://f47c88a44b9d72899d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7866 <> https://13a33e76687569c288.gradio.live\n",
            "Killing tunnel 127.0.0.1:7867 <> https://7597e00ceb5c430b5e.gradio.live\n",
            "Killing tunnel 127.0.0.1:7868 <> https://14abe7c142fdfa7865.gradio.live\n",
            "Killing tunnel 127.0.0.1:7869 <> https://32fd7eede5f6584f5c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# # Define your CSS\n",
        "# css = \"\"\"\n",
        "# h1 {\n",
        "#     margin-top: 2rem;\n",
        "#     font-size: 2rem;\n",
        "#     text-align: center;\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# # Define input fields\n",
        "# input_text = gr.Textbox(label=\"Input Text\", lines=10)\n",
        "# max_words = gr.Slider(label=\"Max Words\", minimum=10, maximum=100, step=10, value=50)\n",
        "# input_pdf_path = gr.File(label=\"Enter the PDF file path\")\n",
        "# # chain_type = gr.Text(label=\"Enter Chain_type\")\n",
        "# output_summary = gr.Text(label=\"Summary\")\n",
        "\n",
        "# # Gradio app structure\n",
        "# with gr.Blocks(title=\"Summarizer App\", css=css) as demo:\n",
        "#     gr.Markdown(\"# Summarizer App\")\n",
        "\n",
        "#     with gr.Tabs():\n",
        "#         with gr.TabItem(\"Extractive\"):\n",
        "#           input_type = gr.Radio(\n",
        "#                         [\"Text Input\", \"PDF Upload\"],\n",
        "#                         label=\"Input Type\",\n",
        "#                         value=\"Text Input\"\n",
        "#                     )\n",
        "\n",
        "#                     # Text Input\n",
        "#                     text_input = gr.Textbox(\n",
        "#                         label=\"Input Text\",\n",
        "#                         lines=5,\n",
        "#                         placeholder=\"Enter text here\",\n",
        "#                         visible=True\n",
        "#                     )\n",
        "\n",
        "#                     # PDF Upload\n",
        "#                     pdf_input = gr.File(\n",
        "#                         label=\"Upload PDF\",\n",
        "#                         type=\"filepath\",\n",
        "#                         file_types=[\".pdf\"],\n",
        "#                         visible=False\n",
        "#                     )\n",
        "#             extractive_dropdown = gr.Dropdown(\n",
        "#                 [\"Frequency Method\", \"Sumy Method\", \"Luhn Method\"],\n",
        "#                 label=\"Choose Extractive Method\",\n",
        "#             )\n",
        "#             gr.Interface(\n",
        "#                 fn=extractive_summarize,\n",
        "#                 inputs=[input_text, extractive_dropdown, max_words],\n",
        "#                 # outputs=[\"text\",\"text\"],\n",
        "#                 outputs=[\n",
        "#                     gr.Textbox(label=\"Summary Output\"),\n",
        "#                     gr.Textbox(label=\"Word Count\"),\n",
        "#                 ],\n",
        "#                 flagging_mode=\"never\",\n",
        "#                 live=False,\n",
        "#             )\n",
        "#         with gr.TabItem(\"Abstractive\"):\n",
        "#             abstractive_dropdown = gr.Dropdown(\n",
        "#                 [\"T5\", \"BART\", \"LLM Model\"],\n",
        "#                 label=\"Choose Abstractive Model\",\n",
        "#             )\n",
        "#             gr.Interface(\n",
        "#                 fn=abstractive_summarize,\n",
        "#                 inputs=[input_text, abstractive_dropdown, max_words],\n",
        "#                 # outputs=\"text\",\n",
        "#                 outputs=[\n",
        "#                     gr.Textbox(label=\"Summary Output\"),\n",
        "#                     gr.Textbox(label=\"Word Count\"),\n",
        "#                 ],\n",
        "#                 flagging_mode=\"never\",\n",
        "#                 live=False,\n",
        "#             )\n",
        "#         with gr.TabItem(\"PDF\"):\n",
        "#               chain_type = gr.Dropdown(\n",
        "#                 [\"refine\", \"map_reduce\"],\n",
        "#                 label=\"Choose Chain_type\",\n",
        "#             )\n",
        "#               gr.Interface(fn=summarize_pdf,\n",
        "#                         inputs=[input_pdf_path,chain_type,max_words],\n",
        "#                         # outputs=[output_summary],\n",
        "#                         outputs=[\n",
        "#                            gr.Textbox(label=\"Summary Output\"),\n",
        "#                            gr.Textbox(label=\"Word Count\"),\n",
        "#                         ],\n",
        "#                         flagging_mode='never',\n",
        "#                         submit_btn='Generate',\n",
        "#                         live=False,\n",
        "#             )\n",
        "\n",
        "# # Launch app\n",
        "# demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmxUgztrr18l97lUsdgCPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}