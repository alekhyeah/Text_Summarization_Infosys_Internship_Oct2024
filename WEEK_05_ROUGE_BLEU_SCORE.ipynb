{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets sumy scikit-learn nltk rouge-score openpyxl\n",
        "!pip install torch transformers datasets\n",
        "!pip install torch transformers datasets nltk sumy scikit-learn sentence-transformers rouge-score openpyxl\n",
        "!pip install sumy\n",
        "!pip install sacrebleu\n",
        "!pip install rouge_score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uzcdfryBzPoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu.metrics import BLEU\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import openai\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sfwT8PQAwjXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(dataset_path=\"./cnn_dailymail.csv\"):\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please ensure the file exists.\")\n",
        "    return pd.read_csv(dataset_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "TJ2UN72uyNaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_summaries(text, summarizers):\n",
        "    summaries = {}\n",
        "    for name, summarizer in summarizers.items():\n",
        "        summaries[name] = summarizer(text) if callable(summarizer) else summarizer(text)\n",
        "    return summaries\n",
        "\n",
        "def summarize_with_llm(text):\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in summarization.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize this article: {text}\"}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "\n",
        "def calculate_scores(reference, candidate):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, candidate)\n",
        "    bleu = BLEU().corpus_score([candidate], [[reference]])\n",
        "    return rouge_scores, bleu\n",
        "\n",
        "\n",
        "def process_and_save(dataset_path, output_excel=\"summaries_scores.xlsx\"):\n",
        "    df = load_dataset(dataset_path)\n",
        "    results = []\n",
        "\n",
        "\n",
        "    transformers_summarizers = {\n",
        "        \"T5\": pipeline(\"summarization\", model=\"t5-small\"),\n",
        "        \"BART\": pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    }\n",
        "    statistical_summarizers = {\n",
        "        \"Luhn\": summarize_with_luhn,\n",
        "        \"KMeans\": summarize_with_kmeans,\n",
        "        \"LexRank\": summarize_with_lexrank\n",
        "    }\n",
        "    llm_summarizers = {\n",
        "        \"LLM-GPT\": summarize_with_llm\n",
        "    }\n",
        "\n",
        "    summarizers = {**transformers_summarizers, **statistical_summarizers, **llm_summarizers}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[\"article\"]\n",
        "        reference = row[\"highlights\"]\n",
        "        summaries = generate_summaries(text, summarizers)\n",
        "\n",
        "        for method, summary in summaries.items():\n",
        "            rouge_scores, bleu_score = calculate_scores(reference, summary)\n",
        "            results.append({\n",
        "                \"Article\": text,\n",
        "                \"Reference\": reference,\n",
        "                \"Method\": method,\n",
        "                \"Summary\": summary,\n",
        "                \"Rouge-1\": rouge_scores['rouge1'].fmeasure,\n",
        "                \"Rouge-2\": rouge_scores['rouge2'].fmeasure,\n",
        "                \"Rouge-L\": rouge_scores['rougeL'].fmeasure,\n",
        "                \"BLEU\": bleu_score.score\n",
        "            })\n",
        "        if index >= 10:\n",
        "            break\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_excel(output_excel, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "iyQOJYdbyNbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summarize_with_luhn(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    return \" \".join(str(sentence) for sentence in summarizer(parser.document, 3))\n",
        "\n",
        "def summarize_with_kmeans(text):\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    sentences = text.split(\". \")\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    kmeans = KMeans(n_clusters=1)\n",
        "    kmeans.fit(X)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    closest = sorted(((i, c) for i, c in enumerate(X.dot(centers[0]))), key=lambda x: x[1], reverse=True)\n",
        "    return \". \".join(sentences[i[0]] for i in closest[:3])\n",
        "\n",
        "def summarize_with_lexrank(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    return \" \".join(str(sentence) for sentence in summarizer(parser.document, 3))\n",
        "\n"
      ],
      "metadata": {
        "id": "xcvUsjtzyNeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"./cnn_dailymail.csv\"\n",
        "    process_and_save(dataset_path, output_excel=\"summary.xlsx\")\n"
      ],
      "metadata": {
        "id": "FFj3z-lEyNgP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}